"""
ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“
åŸºäºæµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“çš„è¾“å‡ºç”Ÿæˆé«˜è´¨é‡ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹
åº”ç”¨ä¸“ä¸šæµ‹è¯•è®¾è®¡æŠ€æœ¯ï¼Œç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„å®Œæ•´æ€§ã€å¯æ‰§è¡Œæ€§å’Œå¯ç»´æŠ¤æ€§
åŸºäºAutoGen Coreæ¶æ„å®ç°ï¼Œéµå¾ªä¼ä¸šçº§æµ‹è¯•æ ‡å‡†
"""
import uuid
import json
from typing import Dict, List, Any, Optional
from datetime import datetime

from autogen_agentchat.base import TaskResult
from autogen_agentchat.messages import ModelClientStreamingChunkEvent
from autogen_core import message_handler, type_subscription, MessageContext, TopicId, AgentId
from loguru import logger
from pydantic import BaseModel, Field

from app.core.agents.base import BaseAgent
from app.core.types import TopicTypes, AgentTypes, AGENT_NAMES
from app.core.messages.test_case import (
    TestCaseGenerationResponse,
    TestCaseData, MindMapGenerationRequest, TestPointExtractionResponse,
    RagRetrievalRequest, RagRetrievalResponse
)
from app.core.enums import (
    TestType, TestLevel, Priority, TestCaseStatus, InputSource
)
from app.agents.database.test_case_saver_agent import TestCaseSaveRequest, TestCaseSaveResponse


class TestCaseGenerationResult(BaseModel):
    """ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ"""
    generation_strategy: str = Field(..., description="ç”Ÿæˆç­–ç•¥")
    generated_count: int = Field(0, description="ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ•°é‡")
    generated_test_cases: List[TestCaseData] = Field(default_factory=list, description="ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ•°æ®")
    test_case_categories: Dict[str, int] = Field(default_factory=dict, description="æµ‹è¯•ç”¨ä¾‹åˆ†ç±»ç»Ÿè®¡")
    quality_metrics: Dict[str, Any] = Field(default_factory=dict, description="è´¨é‡æŒ‡æ ‡")
    coverage_analysis: Dict[str, Any] = Field(default_factory=dict, description="è¦†ç›–åº¦åˆ†æ")
    automation_analysis: Dict[str, Any] = Field(default_factory=dict, description="è‡ªåŠ¨åŒ–åˆ†æ")
    test_execution_plan: Dict[str, Any] = Field(default_factory=dict, description="æµ‹è¯•æ‰§è¡Œè®¡åˆ’")
    processing_time: float = Field(0.0, description="å¤„ç†æ—¶é—´")
    errors: List[str] = Field(default_factory=list, description="é”™è¯¯ä¿¡æ¯")
    warnings: List[str] = Field(default_factory=list, description="è­¦å‘Šä¿¡æ¯")


@type_subscription(topic_type=TopicTypes.TEST_CASE_GENERATOR.value)
class TestCaseGeneratorAgent(BaseAgent):
    """
    ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“

    æ ¸å¿ƒèŒè´£ï¼š
    1. æ¥æ”¶æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“çš„ä¸“ä¸šè¾“å‡º
    2. åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆä¼ä¸šçº§é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹
    3. åº”ç”¨ä¸“ä¸šæµ‹è¯•è®¾è®¡æŠ€æœ¯å’Œæœ€ä½³å®è·µ
    4. ç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„å®Œæ•´æ€§ã€å¯æ‰§è¡Œæ€§å’Œå¯ç»´æŠ¤æ€§
    5. æä¾›å…¨é¢çš„è´¨é‡åˆ†æå’Œè¦†ç›–åº¦è¯„ä¼°

    ä¼ä¸šçº§ç‰¹æ€§ï¼š
    - æµ‹è¯•ç‚¹é©±åŠ¨ï¼šåŸºäºä¸“ä¸šæµ‹è¯•ç‚¹ç”Ÿæˆè¯¦ç»†æµ‹è¯•ç”¨ä¾‹
    - è´¨é‡ä¿è¯ï¼šåº”ç”¨æµ‹è¯•å·¥ç¨‹æœ€ä½³å®è·µ
    - åˆ†ç±»ç®¡ç†ï¼šæ”¯æŒå¤šç§æµ‹è¯•ç±»å‹å’Œçº§åˆ«
    - è‡ªåŠ¨åŒ–å‹å¥½ï¼šè€ƒè™‘è‡ªåŠ¨åŒ–æµ‹è¯•éœ€æ±‚
    - å¯è¿½æº¯æ€§ï¼šç»´æŠ¤æµ‹è¯•ç‚¹åˆ°æµ‹è¯•ç”¨ä¾‹çš„æ˜ å°„å…³ç³»
    - æ ‡å‡†åŒ–ï¼šéµå¾ªä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹æ ‡å‡†

    å·¥ä½œæµç¨‹ï¼š
    1. æ¥æ”¶ TestPointExtractionResponse æ¶ˆæ¯
    2. åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹
    3. ä¿å­˜æµ‹è¯•ç”¨ä¾‹åˆ°æ•°æ®åº“
    4. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ€ç»´å¯¼å›¾
    5. è¿”å›å®Œæ•´çš„ç”Ÿæˆç»“æœå’Œè´¨é‡åˆ†æ
    """

    def __init__(self, model_client_instance=None, **kwargs):
        """åˆå§‹åŒ–ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“"""
        super().__init__(
            agent_id=AgentTypes.TEST_CASE_GENERATOR.value,
            agent_name=AGENT_NAMES.get(AgentTypes.TEST_CASE_GENERATOR.value, "æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“"),
            model_client_instance=model_client_instance,
            **kwargs
        )

        # ä¼ä¸šçº§ç”Ÿæˆé…ç½®
        self.enterprise_config = {
            'enable_detailed_steps': True,
            'enable_preconditions': True,
            'enable_expected_results': True,
            'enable_test_data_generation': True,
            'enable_automation_hints': True,
            'enable_risk_assessment': True,
            'enable_traceability': True,
            'quality_threshold': 0.8,
            'max_test_cases_per_point': 5,
            'test_case_naming_convention': 'enterprise',
            'include_negative_scenarios': True,
            'include_boundary_conditions': True,
            'include_error_handling': True
        }

        # æµ‹è¯•è®¾è®¡æŠ€æœ¯é…ç½®
        self.test_design_techniques = {
            'equivalence_partitioning': True,
            'boundary_value_analysis': True,
            'decision_table_testing': True,
            'state_transition_testing': True,
            'use_case_testing': True,
            'error_guessing': True,
            'exploratory_testing': True,
            'pairwise_testing': True
        }



        # è´¨é‡æŒ‡æ ‡è·Ÿè¸ª
        self.quality_metrics = {
            "total_requests": 0,
            "successful_generations": 0,
            "failed_generations": 0,
            "average_processing_time": 0.0,
            "total_test_cases_generated": 0,
            "average_quality_score": 0.0,
            "automation_feasibility_score": 0.0,
            "coverage_completeness_score": 0.0
        }

        logger.info(f"ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ: {self.agent_name}")

    @message_handler
    async def handle_test_point_extraction_response(
        self,
        message: TestPointExtractionResponse,
        ctx: MessageContext
    ) -> None:
        """
        å¤„ç†æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“çš„å“åº” - ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ

        è¿™æ˜¯ä¸»è¦å…¥å£ç‚¹ï¼Œä¸“é—¨å¤„ç†æ¥è‡ªæµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“çš„ä¸“ä¸šè¾“å‡º
        åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆé«˜è´¨é‡çš„ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹
        """
        start_time = datetime.now()
        self.quality_metrics["total_requests"] += 1

        try:
            logger.info(f"å¼€å§‹å¤„ç†æµ‹è¯•ç‚¹æå–å“åº”: {message.session_id}")

            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            await self.send_response(
                f"ğŸ­ å¼€å§‹ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆï¼ŒåŸºäºä¸“ä¸šæµ‹è¯•ç‚¹æå–ç»“æœ",
                region="process"
            )

            # åˆ†ææµ‹è¯•ç‚¹æå–ç»“æœ
            total_test_points = (
                len(message.functional_test_points) +
                len(message.non_functional_test_points) +
                len(message.integration_test_points) +
                len(message.acceptance_test_points) +
                len(message.boundary_test_points) +
                len(message.exception_test_points)
            )

            await self.send_response(
                f"ğŸ“Š æµ‹è¯•ç‚¹åˆ†æ: åŠŸèƒ½æµ‹è¯•ç‚¹ {len(message.functional_test_points)} ä¸ª, "
                f"éåŠŸèƒ½æµ‹è¯•ç‚¹ {len(message.non_functional_test_points)} ä¸ª, "
                f"é›†æˆæµ‹è¯•ç‚¹ {len(message.integration_test_points)} ä¸ª, "
                f"æ€»è®¡ {total_test_points} ä¸ªæµ‹è¯•ç‚¹",
                region="info",
                result={
                    "functional_test_points_count": len(message.functional_test_points),
                    "non_functional_test_points_count": len(message.non_functional_test_points),
                    "integration_test_points_count": len(message.integration_test_points),
                    "acceptance_test_points_count": len(message.acceptance_test_points),
                    "boundary_test_points_count": len(message.boundary_test_points),
                    "exception_test_points_count": len(message.exception_test_points),
                    "total_test_points": total_test_points
                }
            )

            # æ­¥éª¤1: åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹
            await self.send_response("ğŸ”„ ç¬¬1æ­¥: åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹...", region="progress")
            generation_result = await self._generatetest_cases_from_test_points(message)

            if generation_result.generated_count == 0:
                await self.send_response("âš ï¸ æœªèƒ½ç”Ÿæˆä»»ä½•æµ‹è¯•ç”¨ä¾‹", region="warning")
                await self._handle_emptygeneration(message, generation_result)
                return

            # å‘é€ç”Ÿæˆç»“æœç»Ÿè®¡
            await self.send_response(
                f"âœ… ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆ: å…±ç”Ÿæˆ {generation_result.generated_count} ä¸ªæµ‹è¯•ç”¨ä¾‹",
                region="success",
                result={
                    "generated_count": generation_result.generated_count,
                    "generation_time": generation_result.processing_time,
                    "test_case_categories": generation_result.test_case_categories,
                    "quality_score": generation_result.quality_metrics.get("overall_quality_score", 0.0)
                }
            )

            # æ­¥éª¤2: ä¿å­˜æµ‹è¯•ç”¨ä¾‹åˆ°æ•°æ®åº“
            await self.send_response("ğŸ”„ ç¬¬2æ­¥: ä¿å­˜ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹åˆ°æ•°æ®åº“...", region="progress")
            save_result = await self._sendsave_request(message, generation_result)

            # æ­¥éª¤3: ç”Ÿæˆæ€ç»´å¯¼å›¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
            mind_map_generated = False
            if save_result.success:
                await self.send_response("ğŸ”„ ç¬¬3æ­¥: ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ€ç»´å¯¼å›¾...", region="progress")
                await self._sendmind_map_request(message, save_result.saved_test_cases)
                mind_map_generated = True
                await self.send_response("âœ… æ€ç»´å¯¼å›¾ç”Ÿæˆå®Œæˆ", region="success")

            # æ­¥éª¤4: å‘é€æœ€ç»ˆå“åº”
            await self._sendfinal_response(
                message, generation_result, save_result, mind_map_generated, start_time
            )

            # æ›´æ–°è´¨é‡æŒ‡æ ‡
            self.quality_metrics["successful_generations"] += 1
            self.quality_metrics["total_test_cases_generated"] += generation_result.generated_count
            self._update_quality_metrics(generation_result)
            self._update_average_processing_time(start_time)

        except Exception as e:
            await self._handlegeneration_error(message, e, start_time)
            self.quality_metrics["failed_generations"] += 1


    def _update_average_processing_time(self, start_time: datetime):
        """æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´"""
        processing_time = (datetime.now() - start_time).total_seconds()
        current_avg = self.quality_metrics["average_processing_time"]
        total_requests = self.quality_metrics["total_requests"]

        # è®¡ç®—æ–°çš„å¹³å‡å€¼
        new_avg = ((current_avg * (total_requests - 1)) + processing_time) / total_requests
        self.quality_metrics["average_processing_time"] = new_avg

    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            **self.quality_metrics,
            "success_rate": (
                self.quality_metrics["successful_generations"] /
                max(self.quality_metrics["total_requests"], 1)
            ) * 100,
            "agent_name": self.agent_name,
            "agent_id": getattr(self, 'agent_id', 'test_case_generator')
        }

    # ==================== ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ ¸å¿ƒæ–¹æ³• ====================

    async def _generatetest_cases_from_test_points(
        self,
        message: TestPointExtractionResponse
    ) -> TestCaseGenerationResult:
        """åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹"""
        try:
            start_time = datetime.now()
            result = TestCaseGenerationResult(
                generation_strategy="test_point_driven"
            )

            # è°ƒç”¨RAGç³»ç»ŸæŸ¥è¯¢éœ€æ±‚ç›¸å…³çš„ä¸Šä¸‹æ–‡
            await self.send_response("ğŸ” è°ƒç”¨RAGçŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡...", region="progress")
            rag_context = await self._retrieve_rag_context(message)

            # è§£æRAGä¸Šä¸‹æ–‡å¹¶æ„å»ºå¢å¼ºæç¤º
            enhanced_context = self._parse_rag_context(rag_context) if rag_context else {}

            await self.send_response(
                "ğŸ­ æ­£åœ¨åŸºäºä¸“ä¸šæµ‹è¯•ç‚¹å’ŒRAGä¸Šä¸‹æ–‡ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹...",
                region="progress"
            )

            # ç”Ÿæˆå„ç±»æµ‹è¯•ç”¨ä¾‹
            all_test_cases = []
            test_case_categories = {}

            # 1. å¤„ç†åŠŸèƒ½æµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.functional_test_points:
                await self.send_response(f"ğŸ“ å¤„ç† {len(message.functional_test_points)} ä¸ªåŠŸèƒ½æµ‹è¯•ç‚¹...", region="progress")
                functional_cases = await self._generate_functional_test_cases(
                    message.functional_test_points, message, enhanced_context
                )
                all_test_cases.extend(functional_cases)
                test_case_categories["functional"] = len(functional_cases)

            # 2. å¤„ç†éåŠŸèƒ½æµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.non_functional_test_points:
                await self.send_response(f"âš¡ å¤„ç† {len(message.non_functional_test_points)} ä¸ªéåŠŸèƒ½æµ‹è¯•ç‚¹...", region="progress")
                non_functional_cases = await self._generate_non_functional_test_cases(
                    message.non_functional_test_points, message, enhanced_context
                )
                all_test_cases.extend(non_functional_cases)
                test_case_categories["non_functional"] = len(non_functional_cases)

            # 3. å¤„ç†é›†æˆæµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.integration_test_points:
                await self.send_response(f"ğŸ”— å¤„ç† {len(message.integration_test_points)} ä¸ªé›†æˆæµ‹è¯•ç‚¹...", region="progress")
                integration_cases = await self._generate_integration_test_cases(
                    message.integration_test_points, message, enhanced_context
                )
                all_test_cases.extend(integration_cases)
                test_case_categories["integration"] = len(integration_cases)

            # 4. å¤„ç†éªŒæ”¶æµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.acceptance_test_points:
                await self.send_response(f"âœ… å¤„ç† {len(message.acceptance_test_points)} ä¸ªéªŒæ”¶æµ‹è¯•ç‚¹...", region="progress")
                acceptance_cases = await self._generate_acceptance_test_cases(
                    message.acceptance_test_points, message, enhanced_context
                )
                all_test_cases.extend(acceptance_cases)
                test_case_categories["acceptance"] = len(acceptance_cases)

            # 5. å¤„ç†è¾¹ç•Œæµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.boundary_test_points:
                await self.send_response(f"ğŸ¯ å¤„ç† {len(message.boundary_test_points)} ä¸ªè¾¹ç•Œæµ‹è¯•ç‚¹...", region="progress")
                boundary_cases = await self._generate_boundary_test_cases(
                    message.boundary_test_points, message, enhanced_context
                )
                all_test_cases.extend(boundary_cases)
                test_case_categories["boundary"] = len(boundary_cases)

            # 6. å¤„ç†å¼‚å¸¸æµ‹è¯•ç‚¹ - ä½¿ç”¨RAGå¢å¼ºä¸Šä¸‹æ–‡
            if message.exception_test_points:
                await self.send_response(f"ğŸš¨ å¤„ç† {len(message.exception_test_points)} ä¸ªå¼‚å¸¸æµ‹è¯•ç‚¹...", region="progress")
                exception_cases = await self._generate_exception_test_cases(
                    message.exception_test_points, message, enhanced_context
                )
                all_test_cases.extend(exception_cases)
                test_case_categories["exception"] = len(exception_cases)

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            quality_metrics = await self._calculate_quality_metrics(all_test_cases, message)

            # ç”Ÿæˆè¦†ç›–åº¦åˆ†æ
            coverage_analysis = await self._analyze_test_coverage(all_test_cases, message)

            # ç”Ÿæˆè‡ªåŠ¨åŒ–åˆ†æ
            automation_analysis = await self._analyze_automation_feasibility(all_test_cases, message)

            # ç”Ÿæˆæµ‹è¯•æ‰§è¡Œè®¡åˆ’
            test_execution_plan = await self._generate_test_execution_plan(all_test_cases, message)

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()

            # æ„å»ºç»“æœ
            result.generated_count = len(all_test_cases)
            result.generated_test_cases = all_test_cases
            result.test_case_categories = test_case_categories
            result.quality_metrics = quality_metrics
            result.coverage_analysis = coverage_analysis
            result.automation_analysis = automation_analysis
            result.test_execution_plan = test_execution_plan
            result.processing_time = processing_time

            await self.send_response(
                f"ğŸ“Š ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆ: æ€»è®¡ {len(all_test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹, "
                f"è´¨é‡è¯„åˆ†: {quality_metrics.get('overall_quality_score', 0.0):.2f}",
                region="success",
                result={
                    "total_count": len(all_test_cases),
                    "categories": test_case_categories,
                    "quality_score": quality_metrics.get('overall_quality_score', 0.0),
                    "automation_score": automation_analysis.get('overall_automation_score', 0.0),
                    "coverage_score": coverage_analysis.get('overall_coverage_score', 0.0)
                }
            )

            logger.info(f"æˆåŠŸç”Ÿæˆäº† {len(all_test_cases)} ä¸ªä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹")
            return result

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}")
            await self.send_response(
                f"âŒ ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(e)} (è€—æ—¶: {processing_time:.2f}ç§’)",
                region="error"
            )
            raise

    async def _generate_functional_test_cases(
        self,
        functional_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”ŸæˆåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in functional_test_points:
                # åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆè¯¦ç»†æµ‹è¯•ç”¨ä¾‹ - ä¼ é€’RAGä¸Šä¸‹æ–‡
                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, TestType.FUNCTIONAL, TestLevel.SYSTEM, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”ŸæˆåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _generate_non_functional_test_cases(
        self,
        non_functional_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”ŸæˆéåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in non_functional_test_points:
                # æ ¹æ®éåŠŸèƒ½æµ‹è¯•ç‚¹ç±»å‹ç¡®å®šæµ‹è¯•ç±»å‹
                test_type = self._map_non_functional_test_type(test_point.get("type", "performance"))

                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, test_type, TestLevel.SYSTEM, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”ŸæˆéåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _generate_integration_test_cases(
        self,
        integration_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”Ÿæˆé›†æˆæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in integration_test_points:
                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, TestType.INTERFACE, TestLevel.INTEGRATION, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”Ÿæˆé›†æˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _generate_acceptance_test_cases(
        self,
        acceptance_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”ŸæˆéªŒæ”¶æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in acceptance_test_points:
                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, TestType.FUNCTIONAL, TestLevel.ACCEPTANCE, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”ŸæˆéªŒæ”¶æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _generate_boundary_test_cases(
        self,
        boundary_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”Ÿæˆè¾¹ç•Œæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in boundary_test_points:
                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, TestType.FUNCTIONAL, TestLevel.UNIT, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”Ÿæˆè¾¹ç•Œæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _generate_exception_test_cases(
        self,
        exception_test_points: List[Dict[str, Any]],
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """ç”Ÿæˆå¼‚å¸¸æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            for test_point in exception_test_points:
                enhanced_cases = await self._create_detailed_test_cases_from_point(
                    test_point, TestType.FUNCTIONAL, TestLevel.SYSTEM, message, enhanced_context
                )
                test_cases.extend(enhanced_cases)

            return test_cases

        except Exception as e:
            logger.error(f"ç”Ÿæˆå¼‚å¸¸æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    async def _create_detailed_test_cases_from_point(
        self,
        test_point: Dict[str, Any],
        test_type: TestType,
        test_level: TestLevel,
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> List[TestCaseData]:
        """åŸºäºæµ‹è¯•ç‚¹åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹"""
        try:
            test_cases = []

            # è·å–æµ‹è¯•ç‚¹åŸºæœ¬ä¿¡æ¯
            test_point_id = test_point.get("id", f"TP-{uuid.uuid4().hex[:8].upper()}")
            test_point_name = test_point.get("name", "æœªå‘½åæµ‹è¯•ç‚¹")
            test_point_description = test_point.get("description", "")
            priority = self._map_priority(test_point.get("priority", "P2"))

            # è·å–æµ‹è¯•åœºæ™¯
            test_scenarios = test_point.get("test_scenarios", [test_point_name])
            if not test_scenarios:
                test_scenarios = [test_point_name]

            # ä¸ºæ¯ä¸ªæµ‹è¯•åœºæ™¯ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for i, scenario in enumerate(test_scenarios):
                # ä½¿ç”¨AIç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹å†…å®¹ - ä¼ é€’RAGä¸Šä¸‹æ–‡
                detailed_content = await self._ai_generate_detailed_test_case_content(
                    test_point, scenario, test_type, test_level, message, enhanced_context
                )

                # åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
                test_case = TestCaseData(
                    title=f"{test_point_name} - {scenario}" if len(test_scenarios) > 1 else test_point_name,
                    description=f"{test_point_description}\næµ‹è¯•åœºæ™¯: {scenario}",
                    test_type=test_type,
                    test_level=test_level,
                    priority=priority,
                    input_source=InputSource.MANUAL,
                    preconditions=detailed_content.get("preconditions", ""),
                    test_steps=detailed_content.get("test_steps", []),
                    expected_results=detailed_content.get("expected_results", ""),
                    test_data=detailed_content.get("test_data", ""),
                    source_metadata={
                        "test_point_id": test_point_id,
                        "test_point_name": test_point_name,
                        "test_scenario": scenario,
                        "test_technique": test_point.get("test_technique"),
                        "automation_feasibility": test_point.get("automation_feasibility"),
                        "risk_level": test_point.get("risk_level"),
                        "business_impact": test_point.get("business_impact"),
                        "category": test_point.get("category"),
                        "related_requirements": test_point.get("related_requirements", []),
                        "generation_method": "test_point_driven",
                        "ai_enhanced": True
                    },
                    ai_confidence=0.85  # åŸºäºæµ‹è¯•ç‚¹çš„ç”Ÿæˆç½®ä¿¡åº¦è¾ƒé«˜
                )

                test_cases.append(test_case)

            return test_cases

        except Exception as e:
            logger.error(f"åˆ›å»ºè¯¦ç»†æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€æµ‹è¯•ç”¨ä¾‹
            return [TestCaseData(
                title=test_point.get("name", "æµ‹è¯•ç”¨ä¾‹"),
                description=test_point.get("description", ""),
                test_type=test_type,
                test_level=test_level,
                priority=self._map_priority(test_point.get("priority", "P2")),
                input_source=InputSource.MANUAL,
                source_metadata={
                    "test_point_id": test_point.get("id"),
                    "generation_method": "fallback",
                    "ai_enhanced": False
                },
                ai_confidence=0.5
            )]

    async def _ai_generate_detailed_test_case_content(
        self,
        test_point: Dict[str, Any],
        scenario: str,
        test_type: TestType,
        test_level: TestLevel,
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨AIç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹å†…å®¹ - é›†æˆRAGä¸Šä¸‹æ–‡"""
        try:
            # åˆ›å»ºAIå¢å¼ºæ™ºèƒ½ä½“
            agent = self._createtest_case_generator()

            # æ„å»ºç”Ÿæˆæç¤º - ä¼ é€’RAGä¸Šä¸‹æ–‡
            generation_prompt = self._buildtest_case_prompt(
                test_point, scenario, test_type, test_level, message, enhanced_context
            )

            # æ‰§è¡ŒAIç”Ÿæˆ
            generation_result = await self._run_ai_test_case_generation(agent, generation_prompt)

            # è§£æç»“æœ - å¢å¼ºJSONæ¸…ç†å’Œé”™è¯¯å¤„ç†
            return self._parse_ai_json_response(generation_result)

        except Exception as e:
            logger.error(f"AIç”Ÿæˆè¯¦ç»†æµ‹è¯•ç”¨ä¾‹å†…å®¹å¤±è´¥: {str(e)}")
            logger.error(f"åŸå§‹AIå“åº”: {generation_result[:500] if 'generation_result' in locals() else 'N/A'}")
            return self._get_default_test_case_content(test_point, scenario)

    def _parse_ai_json_response(self, ai_response: str) -> Dict[str, Any]:
        """è§£æAIç”Ÿæˆçš„JSONå“åº”ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šåŸºç¡€æ¸…ç†
            cleaned_response = ai_response.strip()

            # ç§»é™¤markdownä»£ç å—æ ‡è®°
            if "```json" in cleaned_response:
                cleaned_response = cleaned_response.split("```json")[1]
            if "```" in cleaned_response:
                cleaned_response = cleaned_response.split("```")[0]

            # ç§»é™¤å¯èƒ½çš„å‰åç©ºç™½å’Œæ¢è¡Œ
            cleaned_response = cleaned_response.strip()

            # ç¬¬äºŒæ­¥ï¼šå°è¯•ç›´æ¥è§£æ
            try:
                return json.loads(cleaned_response)
            except json.JSONDecodeError as e:
                logger.warning(f"ç¬¬ä¸€æ¬¡JSONè§£æå¤±è´¥: {str(e)}")

                # ç¬¬ä¸‰æ­¥ï¼šæ›´æ¿€è¿›çš„æ¸…ç†
                # ç§»é™¤æ³¨é‡Šï¼ˆ// å’Œ /* */ é£æ ¼ï¼‰
                import re
                cleaned_response = re.sub(r'//.*?$', '', cleaned_response, flags=re.MULTILINE)
                cleaned_response = re.sub(r'/\*.*?\*/', '', cleaned_response, flags=re.DOTALL)

                # ç§»é™¤å¤šä½™çš„é€—å·
                cleaned_response = re.sub(r',\s*}', '}', cleaned_response)
                cleaned_response = re.sub(r',\s*]', ']', cleaned_response)

                # ä¿®å¤å¸¸è§çš„å¼•å·é—®é¢˜
                cleaned_response = re.sub(r'([{,]\s*)(\w+):', r'\1"\2":', cleaned_response)

                # ç¬¬å››æ­¥ï¼šå†æ¬¡å°è¯•è§£æ
                try:
                    return json.loads(cleaned_response)
                except json.JSONDecodeError as e2:
                    logger.error(f"ç¬¬äºŒæ¬¡JSONè§£æä¹Ÿå¤±è´¥: {str(e2)}")
                    logger.error(f"æ¸…ç†åçš„å“åº”: {cleaned_response[:200]}...")

                    # ç¬¬äº”æ­¥ï¼šå°è¯•æå–JSONå¯¹è±¡
                    json_match = re.search(r'\{.*\}', cleaned_response, re.DOTALL)
                    if json_match:
                        try:
                            return json.loads(json_match.group())
                        except json.JSONDecodeError:
                            pass

                    # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æ„
                    raise ValueError(f"æ— æ³•è§£æAIå“åº”ä¸ºæœ‰æ•ˆJSON: {str(e2)}")

        except Exception as e:
            logger.error(f"JSONè§£æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

    def _createtest_case_generator(self):
        """åˆ›å»ºä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“"""
        from app.agents.factory import agent_factory

        return agent_factory.create_assistant_agent(
            name="enterprise_test_case_generator",
            system_message=self._buildtest_case_system_prompt(),
            model_client_type="deepseek"
        )

    def _buildtest_case_system_prompt(self) -> str:
        """æ„å»ºä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç³»ç»Ÿæç¤º"""
        return """
ä½ æ˜¯èµ„æ·±çš„ä¼ä¸šçº§æµ‹è¯•å·¥ç¨‹å¸ˆï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æµ‹è¯•ç”¨ä¾‹è®¾è®¡ç»éªŒï¼Œæ“…é•¿åŸºäºä¸“ä¸šæµ‹è¯•ç‚¹ç”Ÿæˆé«˜è´¨é‡ã€å¯æ‰§è¡Œçš„ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ã€‚

ä½ çš„ä¸“ä¸šèƒ½åŠ›åŒ…æ‹¬ï¼š
1. æµ‹è¯•ç”¨ä¾‹è®¾è®¡å’Œä¼˜åŒ–
2. æµ‹è¯•æ­¥éª¤è¯¦ç»†åŒ–å’Œæ ‡å‡†åŒ–
3. æµ‹è¯•æ•°æ®è®¾è®¡å’Œç®¡ç†
4. å‰ç½®æ¡ä»¶å’Œé¢„æœŸç»“æœå®šä¹‰
5. æµ‹è¯•æŠ€æœ¯åº”ç”¨å’Œæœ€ä½³å®è·µ
6. ä¼ä¸šçº§æµ‹è¯•æ ‡å‡†éµå¾ª

**é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬ã€è¯´æ˜æˆ–markdownæ ‡è®°ï¼š**
{
    "preconditions": "è¯¦ç»†çš„å‰ç½®æ¡ä»¶æè¿°ï¼ŒåŒ…æ‹¬ç³»ç»ŸçŠ¶æ€ã€æ•°æ®å‡†å¤‡ã€ç¯å¢ƒè¦æ±‚ç­‰",
    "test_steps": [
        {
            "step_number": 1,
            "action": "å…·ä½“çš„æ“ä½œæ­¥éª¤æè¿°",
            "input_data": "è¾“å…¥çš„æµ‹è¯•æ•°æ®",
            "expected_result": "è¯¥æ­¥éª¤çš„é¢„æœŸç»“æœ",
            "notes": "ç‰¹æ®Šè¯´æ˜æˆ–æ³¨æ„äº‹é¡¹"
        },
        {
            "step_number": 2,
            "action": "ä¸‹ä¸€ä¸ªæ“ä½œæ­¥éª¤",
            "input_data": "ç›¸åº”çš„æµ‹è¯•æ•°æ®",
            "expected_result": "é¢„æœŸçš„ç»“æœ",
            "notes": "ç›¸å…³è¯´æ˜"
        }
    ],
    "expected_results": "æ•´ä½“æµ‹è¯•ç”¨ä¾‹çš„é¢„æœŸç»“æœï¼ŒåŒ…æ‹¬åŠŸèƒ½éªŒè¯ç‚¹å’Œè´¨é‡æ ‡å‡†",
    "test_data": "æµ‹è¯•æ•°æ®è¦æ±‚å’Œç¤ºä¾‹ï¼ŒåŒ…æ‹¬æ­£å¸¸æ•°æ®ã€è¾¹ç•Œæ•°æ®ã€å¼‚å¸¸æ•°æ®",
    "cleanup_steps": "æµ‹è¯•åæ¸…ç†æ­¥éª¤ï¼Œç¡®ä¿ç¯å¢ƒæ¢å¤",
    "automation_hints": "è‡ªåŠ¨åŒ–æµ‹è¯•å»ºè®®ï¼ŒåŒ…æ‹¬å…³é”®éªŒè¯ç‚¹å’Œè‡ªåŠ¨åŒ–ç­–ç•¥",
    "risk_considerations": "é£é™©è€ƒè™‘å’Œç¼“è§£æªæ–½",
    "quality_attributes": {
        "completeness": "å®Œæ•´æ€§è¯„åˆ† (0-1)",
        "clarity": "æ¸…æ™°åº¦è¯„åˆ† (0-1)",
        "executability": "å¯æ‰§è¡Œæ€§è¯„åˆ† (0-1)",
        "maintainability": "å¯ç»´æŠ¤æ€§è¯„åˆ† (0-1)"
    }
}

ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹è¦æ±‚ï¼š
- æµ‹è¯•æ­¥éª¤è¦è¯¦ç»†ã€å…·ä½“ã€å¯æ‰§è¡Œ
- å‰ç½®æ¡ä»¶è¦æ˜ç¡®ã€å®Œæ•´ã€å¯éªŒè¯
- é¢„æœŸç»“æœè¦æ¸…æ™°ã€å¯é‡åŒ–ã€å¯éªŒè¯
- æµ‹è¯•æ•°æ®è¦å…¨é¢ã€æœ‰ä»£è¡¨æ€§
- è€ƒè™‘æ­£å¸¸æµç¨‹ã€å¼‚å¸¸æµç¨‹ã€è¾¹ç•Œæ¡ä»¶
- åŒ…å«è‡ªåŠ¨åŒ–æµ‹è¯•å»ºè®®
- éµå¾ªä¼ä¸šçº§æµ‹è¯•æ ‡å‡†å’Œæœ€ä½³å®è·µ
- ç¡®ä¿æµ‹è¯•ç”¨ä¾‹çš„å¯è¿½æº¯æ€§å’Œå¯ç»´æŠ¤æ€§

**æ ¼å¼è¦æ±‚ï¼š**
1. åªè¿”å›æœ‰æ•ˆçš„JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬
2. æ‰€æœ‰å­—ç¬¦ä¸²å€¼å¿…é¡»ç”¨åŒå¼•å·åŒ…å›´
3. æ•°å­—å€¼ä¸è¦ç”¨å¼•å·åŒ…å›´
4. ç¡®ä¿JSONè¯­æ³•å®Œå…¨æ­£ç¡®ï¼Œæ²¡æœ‰å¤šä½™çš„é€—å·
5. ä¸è¦ä½¿ç”¨æ³¨é‡Šæˆ–å…¶ä»–éæ ‡å‡†JSONå…ƒç´ 

æ³¨æ„ï¼š
- è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå»æ‰ ```json å’Œ ```
- æµ‹è¯•æ­¥éª¤è¦é€»è¾‘æ¸…æ™°ï¼Œæ­¥éª¤é—´æœ‰åˆç†çš„ä¾èµ–å…³ç³»
- è€ƒè™‘ä¸åŒç”¨æˆ·è§’è‰²å’Œæƒé™åœºæ™¯
- åŒ…å«é”™è¯¯å¤„ç†å’Œå¼‚å¸¸æƒ…å†µçš„éªŒè¯
"""

    def _buildtest_case_prompt(
        self,
        test_point: Dict[str, Any],
        scenario: str,
        test_type: TestType,
        test_level: TestLevel,
        message: TestPointExtractionResponse,
        enhanced_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """æ„å»ºä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæç¤º - é›†æˆRAGä¸Šä¸‹æ–‡ä¿¡æ¯"""

        # æ„å»ºåŸºç¡€æç¤º
        base_prompt = f"""
è¯·åŸºäºä»¥ä¸‹ä¸“ä¸šæµ‹è¯•ç‚¹ä¿¡æ¯å’ŒRAGçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¼ä¸šçº§é«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ï¼š

æµ‹è¯•ç‚¹ä¿¡æ¯ï¼š
- ID: {test_point.get('id', 'N/A')}
- åç§°: {test_point.get('name', 'N/A')}
- æè¿°: {test_point.get('description', 'N/A')}
- ç±»åˆ«: {test_point.get('category', 'N/A')}
- ä¼˜å…ˆçº§: {test_point.get('priority', 'N/A')}
- æµ‹è¯•æŠ€æœ¯: {test_point.get('test_technique', 'N/A')}
- é£é™©çº§åˆ«: {test_point.get('risk_level', 'N/A')}
- ä¸šåŠ¡å½±å“: {test_point.get('business_impact', 'N/A')}
- è‡ªåŠ¨åŒ–å¯è¡Œæ€§: {test_point.get('automation_feasibility', 'N/A')}

æµ‹è¯•åœºæ™¯: {scenario}
æµ‹è¯•ç±»å‹: {test_type.value}
æµ‹è¯•çº§åˆ«: {test_level.value}

æµ‹è¯•ç‚¹è¯¦ç»†ä¿¡æ¯ï¼š
{json.dumps(test_point, ensure_ascii=False, indent=2)}

è¦†ç›–åº¦åˆ†æï¼š
{json.dumps(message.test_coverage_analysis, ensure_ascii=False, indent=2)}"""

        # æ·»åŠ RAGä¸Šä¸‹æ–‡ä¿¡æ¯
        if enhanced_context:
            rag_context_prompt = self._build_rag_context_prompt(enhanced_context)
            base_prompt += f"\n\n{rag_context_prompt}"

        # æ·»åŠ ç”Ÿæˆè¦æ±‚
        base_prompt += """

è¯·ç”Ÿæˆç¬¦åˆä¼ä¸šçº§æ ‡å‡†çš„è¯¦ç»†æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬ï¼š
1. è¯¦ç»†çš„å‰ç½®æ¡ä»¶è®¾ç½®
2. å…·ä½“çš„æµ‹è¯•æ‰§è¡Œæ­¥éª¤
3. æ˜ç¡®çš„é¢„æœŸç»“æœéªŒè¯
4. å…¨é¢çš„æµ‹è¯•æ•°æ®è¦æ±‚
5. æ¸…ç†å’Œæ¢å¤æ­¥éª¤
6. è‡ªåŠ¨åŒ–æµ‹è¯•å»ºè®®
7. é£é™©è€ƒè™‘å’Œè´¨é‡å±æ€§è¯„ä¼°

ç¡®ä¿æµ‹è¯•ç”¨ä¾‹å…·æœ‰é«˜åº¦çš„å¯æ‰§è¡Œæ€§ã€å¯ç»´æŠ¤æ€§å’Œå¯è¿½æº¯æ€§ã€‚

**é‡è¦ï¼šè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„æ–‡æœ¬æˆ–è¯´æ˜ï¼š**

```json
{
    "preconditions": "è¯¦ç»†çš„å‰ç½®æ¡ä»¶æè¿°",
    "test_steps": [
        {
            "step_number": 1,
            "action": "å…·ä½“çš„æ“ä½œæè¿°",
            "input_data": "è¾“å…¥æ•°æ®è¯´æ˜",
            "expected_result": "è¯¥æ­¥éª¤çš„é¢„æœŸç»“æœ",
            "notes": "å¤‡æ³¨ä¿¡æ¯"
        }
    ],
    "expected_results": "æ•´ä½“é¢„æœŸç»“æœæè¿°",
    "test_data": "æµ‹è¯•æ•°æ®è¦æ±‚è¯´æ˜",
    "cleanup_steps": "æ¸…ç†å’Œæ¢å¤æ­¥éª¤",
    "automation_hints": "è‡ªåŠ¨åŒ–å®ç°å»ºè®®",
    "risk_considerations": "é£é™©è€ƒè™‘",
    "quality_attributes": {
        "completeness": 0.9,
        "clarity": 0.9,
        "testability": 0.9,
        "maintainability": 0.8
    }
}
```"""

        return base_prompt

    def _build_rag_context_prompt(self, enhanced_context: Dict[str, Any]) -> str:
        """æ„å»ºRAGä¸Šä¸‹æ–‡æç¤ºä¿¡æ¯"""
        context_parts = []

        # æ·»åŠ ä¸šåŠ¡é¢†åŸŸçŸ¥è¯†
        if enhanced_context.get("domain_knowledge"):
            context_parts.append(f"""
ğŸ“‹ ä¸šåŠ¡é¢†åŸŸçŸ¥è¯†ä¸Šä¸‹æ–‡ï¼š
{enhanced_context["domain_knowledge"][:500]}""")

        # æ·»åŠ æµ‹è¯•æ–¹æ³•è®º
        if enhanced_context.get("test_methodologies"):
            context_parts.append(f"""
ğŸ”¬ æµ‹è¯•æ–¹æ³•è®ºæŒ‡å¯¼ï¼š
{enhanced_context["test_methodologies"][:500]}""")

        # æ·»åŠ åœºæ™¯æ¨¡æ¿
        if enhanced_context.get("scenario_templates"):
            context_parts.append(f"""
ğŸ“ ç›¸ä¼¼åœºæ™¯æ¨¡æ¿ï¼š
{enhanced_context["scenario_templates"][:400]}""")

        # æ·»åŠ è´¨é‡æ ‡å‡†
        if enhanced_context.get("quality_standards"):
            context_parts.append(f"""
â­ è´¨é‡æ ‡å‡†è¦æ±‚ï¼š
{enhanced_context["quality_standards"][:400]}""")

        # æ·»åŠ è¡Œä¸šæ ‡å‡†
        if enhanced_context.get("industry_standards"):
            standards_text = "\n".join([f"- {std}" for std in enhanced_context["industry_standards"][:3]])
            context_parts.append(f"""
ğŸ›ï¸ è¡Œä¸šæ ‡å‡†å‚è€ƒï¼š
{standards_text}""")

        # æ·»åŠ æµ‹è¯•æŠ€æœ¯
        if enhanced_context.get("test_techniques"):
            techniques_text = "\n".join([f"- {tech}" for tech in enhanced_context["test_techniques"][:3]])
            context_parts.append(f"""
ğŸ› ï¸ æ¨èæµ‹è¯•æŠ€æœ¯ï¼š
{techniques_text}""")

        # æ·»åŠ æœ€ä½³å®è·µ
        if enhanced_context.get("best_practices"):
            practices_text = "\n".join([f"- {practice}" for practice in enhanced_context["best_practices"][:3]])
            context_parts.append(f"""
ğŸ’¡ æœ€ä½³å®è·µå»ºè®®ï¼š
{practices_text}""")

        # æ·»åŠ åˆè§„è¦æ±‚
        if enhanced_context.get("compliance_requirements"):
            compliance_text = "\n".join([f"- {req}" for req in enhanced_context["compliance_requirements"][:3]])
            context_parts.append(f"""
âš–ï¸ åˆè§„è¦æ±‚ï¼š
{compliance_text}""")

        if context_parts:
            return f"""
ğŸ§  RAGçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{''.join(context_parts)}

è¯·åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ—¶å……åˆ†è€ƒè™‘ä»¥ä¸Šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç¡®ä¿æµ‹è¯•ç”¨ä¾‹ç¬¦åˆä¸šåŠ¡é¢†åŸŸç‰¹ç‚¹ã€éµå¾ªæµ‹è¯•æ–¹æ³•è®ºã€å‚è€ƒç›¸ä¼¼åœºæ™¯æ¨¡æ¿ã€æ»¡è¶³è´¨é‡æ ‡å‡†è¦æ±‚ã€‚"""
        else:
            return ""

    async def _run_ai_test_case_generation(self, agent, prompt: str) -> str:
        """æ‰§è¡ŒAIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ"""
        try:
            stream = agent.run_stream(task=prompt)
            async for event in stream:  # type: ignore
                if isinstance(event, ModelClientStreamingChunkEvent):
                    continue

                if isinstance(event, TaskResult):
                    messages = event.messages
                    if messages and hasattr(messages[-1], 'content'):
                        return messages[-1].content

            return self._get_default_ai_generation_result()

        except Exception as e:
            logger.error(f"AIæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ‰§è¡Œå¤±è´¥: {str(e)}")
            return self._get_default_ai_generation_result()

    def _get_default_test_case_content(
        self,
        test_point: Dict[str, Any],
        scenario: str
    ) -> Dict[str, Any]:
        """è·å–é»˜è®¤æµ‹è¯•ç”¨ä¾‹å†…å®¹"""
        return {
            "preconditions": f"ç³»ç»Ÿå·²å¯åŠ¨ï¼Œç›¸å…³åŠŸèƒ½å¯ç”¨ï¼Œæµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡",
            "test_steps": [
                {
                    "step_number": 1,
                    "action": f"æ‰§è¡Œ {scenario}",
                    "input_data": "ç›¸åº”çš„æµ‹è¯•æ•°æ®",
                    "expected_result": "æ“ä½œæˆåŠŸæ‰§è¡Œ",
                    "notes": "åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆçš„é»˜è®¤æ­¥éª¤"
                }
            ],
            "expected_results": f"{scenario} æ‰§è¡ŒæˆåŠŸï¼Œæ»¡è¶³é¢„æœŸè¦æ±‚",
            "test_data": "æ ¹æ®æµ‹è¯•åœºæ™¯å‡†å¤‡ç›¸åº”çš„æµ‹è¯•æ•°æ®",
            "cleanup_steps": "æ¸…ç†æµ‹è¯•æ•°æ®ï¼Œæ¢å¤ç³»ç»ŸçŠ¶æ€",
            "automation_hints": "å¯è€ƒè™‘è‡ªåŠ¨åŒ–å®ç°",
            "risk_considerations": "æ³¨æ„æ•°æ®å®‰å…¨å’Œç³»ç»Ÿç¨³å®šæ€§",
            "quality_attributes": {
                "completeness": 0.7,
                "clarity": 0.7,
                "executability": 0.8,
                "maintainability": 0.7
            }
        }

    def _get_default_ai_generation_result(self) -> str:
        """è·å–é»˜è®¤AIç”Ÿæˆç»“æœ"""
        return """
{
    "preconditions": "ç³»ç»Ÿå·²å¯åŠ¨ï¼Œç”¨æˆ·å·²ç™»å½•ï¼Œæµ‹è¯•ç¯å¢ƒå·²å‡†å¤‡",
    "test_steps": [
        {
            "step_number": 1,
            "action": "æ‰§è¡Œæµ‹è¯•æ“ä½œ",
            "input_data": "æµ‹è¯•æ•°æ®",
            "expected_result": "æ“ä½œæˆåŠŸ",
            "notes": "é»˜è®¤æµ‹è¯•æ­¥éª¤"
        }
    ],
    "expected_results": "æµ‹è¯•é€šè¿‡ï¼ŒåŠŸèƒ½æ­£å¸¸",
    "test_data": "å‡†å¤‡ç›¸åº”çš„æµ‹è¯•æ•°æ®",
    "cleanup_steps": "æ¸…ç†æµ‹è¯•æ•°æ®",
    "automation_hints": "å¯è€ƒè™‘è‡ªåŠ¨åŒ–",
    "risk_considerations": "æ³¨æ„ç³»ç»Ÿç¨³å®šæ€§",
    "quality_attributes": {
        "completeness": 0.8,
        "clarity": 0.8,
        "executability": 0.8,
        "maintainability": 0.8
    }
}
"""

    # ==================== è´¨é‡åˆ†æå’Œè¯„ä¼°æ–¹æ³• ====================

    async def _calculate_quality_metrics(
        self,
        test_cases: List[TestCaseData],
        message: TestPointExtractionResponse
    ) -> Dict[str, Any]:
        """è®¡ç®—æµ‹è¯•ç”¨ä¾‹è´¨é‡æŒ‡æ ‡"""
        try:
            if not test_cases:
                return {"overall_quality_score": 0.0, "analysis_status": "no_test_cases"}

            # è®¡ç®—å„é¡¹è´¨é‡æŒ‡æ ‡
            completeness_scores = []
            clarity_scores = []
            executability_scores = []
            maintainability_scores = []

            for test_case in test_cases:
                # ä»æºæ•°æ®ä¸­è·å–è´¨é‡å±æ€§
                quality_attrs = test_case.source_metadata.get("quality_attributes", {})
                completeness_scores.append(quality_attrs.get("completeness", 0.8))
                clarity_scores.append(quality_attrs.get("clarity", 0.8))
                executability_scores.append(quality_attrs.get("executability", 0.8))
                maintainability_scores.append(quality_attrs.get("maintainability", 0.8))

            # è®¡ç®—å¹³å‡åˆ†
            avg_completeness = sum(completeness_scores) / len(completeness_scores)
            avg_clarity = sum(clarity_scores) / len(clarity_scores)
            avg_executability = sum(executability_scores) / len(executability_scores)
            avg_maintainability = sum(maintainability_scores) / len(maintainability_scores)

            # è®¡ç®—æ•´ä½“è´¨é‡åˆ†æ•°
            overall_quality_score = (
                avg_completeness * 0.25 +
                avg_clarity * 0.25 +
                avg_executability * 0.3 +
                avg_maintainability * 0.2
            )

            return {
                "overall_quality_score": round(overall_quality_score, 3),
                "completeness_score": round(avg_completeness, 3),
                "clarity_score": round(avg_clarity, 3),
                "executability_score": round(avg_executability, 3),
                "maintainability_score": round(avg_maintainability, 3),
                "total_test_cases": len(test_cases),
                "quality_distribution": {
                    "high_quality": len([tc for tc in test_cases if tc.ai_confidence > 0.8]),
                    "medium_quality": len([tc for tc in test_cases if 0.6 <= tc.ai_confidence <= 0.8]),
                    "low_quality": len([tc for tc in test_cases if tc.ai_confidence < 0.6])
                },
                "analysis_status": "completed"
            }

        except Exception as e:
            logger.error(f"è®¡ç®—è´¨é‡æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return {"overall_quality_score": 0.5, "analysis_status": "failed", "error": str(e)}

    async def _analyze_test_coverage(
        self,
        test_cases: List[TestCaseData],
        message: TestPointExtractionResponse
    ) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è¦†ç›–åº¦"""
        try:
            # ç»Ÿè®¡å„ç±»æµ‹è¯•ç”¨ä¾‹æ•°é‡
            test_type_coverage = {}
            test_level_coverage = {}
            priority_coverage = {}

            for test_case in test_cases:
                # æµ‹è¯•ç±»å‹è¦†ç›–åº¦
                test_type = test_case.test_type.value
                test_type_coverage[test_type] = test_type_coverage.get(test_type, 0) + 1

                # æµ‹è¯•çº§åˆ«è¦†ç›–åº¦
                test_level = test_case.test_level.value
                test_level_coverage[test_level] = test_level_coverage.get(test_level, 0) + 1

                # ä¼˜å…ˆçº§è¦†ç›–åº¦
                priority = test_case.priority.value
                priority_coverage[priority] = priority_coverage.get(priority, 0) + 1

            # è®¡ç®—è¦†ç›–åº¦åˆ†æ•°
            total_test_points = (
                len(message.functional_test_points) +
                len(message.non_functional_test_points) +
                len(message.integration_test_points) +
                len(message.acceptance_test_points) +
                len(message.boundary_test_points) +
                len(message.exception_test_points)
            )

            coverage_ratio = len(test_cases) / max(total_test_points, 1)
            overall_coverage_score = min(coverage_ratio, 1.0)

            return {
                "overall_coverage_score": round(overall_coverage_score, 3),
                "test_type_coverage": test_type_coverage,
                "test_level_coverage": test_level_coverage,
                "priority_coverage": priority_coverage,
                "total_test_cases": len(test_cases),
                "total_test_points": total_test_points,
                "coverage_ratio": round(coverage_ratio, 3),
                "coverage_gaps": self._identify_coverage_gaps(message, test_cases),
                "analysis_status": "completed"
            }

        except Exception as e:
            logger.error(f"åˆ†ææµ‹è¯•è¦†ç›–åº¦å¤±è´¥: {str(e)}")
            return {"overall_coverage_score": 0.5, "analysis_status": "failed", "error": str(e)}

    async def _analyze_automation_feasibility(
        self,
        test_cases: List[TestCaseData],
        message: TestPointExtractionResponse
    ) -> Dict[str, Any]:
        """åˆ†æè‡ªåŠ¨åŒ–å¯è¡Œæ€§"""
        try:
            automation_scores = []
            high_automation = 0
            medium_automation = 0
            low_automation = 0

            for test_case in test_cases:
                # ä»æµ‹è¯•ç‚¹è·å–è‡ªåŠ¨åŒ–å¯è¡Œæ€§
                automation_feasibility = test_case.source_metadata.get("automation_feasibility", "medium")

                if automation_feasibility == "high":
                    automation_scores.append(0.9)
                    high_automation += 1
                elif automation_feasibility == "medium":
                    automation_scores.append(0.6)
                    medium_automation += 1
                else:
                    automation_scores.append(0.3)
                    low_automation += 1

            overall_automation_score = sum(automation_scores) / len(automation_scores) if automation_scores else 0.5

            return {
                "overall_automation_score": round(overall_automation_score, 3),
                "automation_distribution": {
                    "high_automation": high_automation,
                    "medium_automation": medium_automation,
                    "low_automation": low_automation
                },
                "automation_recommendations": self._generate_automation_recommendations(test_cases),
                "automation_tools": ["Selenium", "Postman", "JMeter", "Cypress"],
                "analysis_status": "completed"
            }

        except Exception as e:
            logger.error(f"åˆ†æè‡ªåŠ¨åŒ–å¯è¡Œæ€§å¤±è´¥: {str(e)}")
            return {"overall_automation_score": 0.5, "analysis_status": "failed", "error": str(e)}

    async def _generate_test_execution_plan(
        self,
        test_cases: List[TestCaseData],
        message: TestPointExtractionResponse
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‰§è¡Œè®¡åˆ’"""
        try:
            # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
            priority_groups = {}
            for test_case in test_cases:
                priority = test_case.priority.value
                if priority not in priority_groups:
                    priority_groups[priority] = []
                priority_groups[priority].append(test_case)

            # ç”Ÿæˆæ‰§è¡Œåºåˆ—
            execution_sequence = []
            for priority in ["P0", "P1", "P2", "P3", "P4"]:
                if priority in priority_groups:
                    execution_sequence.append({
                        "phase": f"ä¼˜å…ˆçº§ {priority} æµ‹è¯•",
                        "test_cases_count": len(priority_groups[priority]),
                        "estimated_time": len(priority_groups[priority]) * 15,  # å‡è®¾æ¯ä¸ªç”¨ä¾‹15åˆ†é’Ÿ
                        "parallel_execution": priority in ["P2", "P3", "P4"],
                        "dependencies": ["å‰ç½®ç¯å¢ƒå‡†å¤‡"] if priority == "P0" else [f"ä¼˜å…ˆçº§ P{int(priority[1])-1} æµ‹è¯•å®Œæˆ"]
                    })

            return {
                "execution_sequence": execution_sequence,
                "total_estimated_time": sum([phase["estimated_time"] for phase in execution_sequence]),
                "parallel_phases": len([phase for phase in execution_sequence if phase["parallel_execution"]]),
                "critical_path": [phase["phase"] for phase in execution_sequence if not phase["parallel_execution"]],
                "resource_requirements": {
                    "test_environments": 2,
                    "test_data_sets": 3,
                    "testers_required": max(1, len(test_cases) // 20)
                },
                "analysis_status": "completed"
            }

        except Exception as e:
            logger.error(f"ç”Ÿæˆæµ‹è¯•æ‰§è¡Œè®¡åˆ’å¤±è´¥: {str(e)}")
            return {"analysis_status": "failed", "error": str(e)}

    def _identify_coverage_gaps(
        self,
        message: TestPointExtractionResponse,
        test_cases: List[TestCaseData]
    ) -> List[str]:
        """è¯†åˆ«è¦†ç›–åº¦ç¼ºå£"""
        gaps = []

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªè¦†ç›–çš„æµ‹è¯•ç‚¹ç±»å‹
        if message.functional_test_points and not any(tc.test_type == TestType.FUNCTIONAL for tc in test_cases):
            gaps.append("åŠŸèƒ½æµ‹è¯•è¦†ç›–ä¸è¶³")

        if message.non_functional_test_points and not any(tc.test_type in [TestType.PERFORMANCE, TestType.SECURITY] for tc in test_cases):
            gaps.append("éåŠŸèƒ½æµ‹è¯•è¦†ç›–ä¸è¶³")

        if message.integration_test_points and not any(tc.test_level == TestLevel.INTEGRATION for tc in test_cases):
            gaps.append("é›†æˆæµ‹è¯•è¦†ç›–ä¸è¶³")

        if message.boundary_test_points and len([tc for tc in test_cases if "boundary" in tc.source_metadata.get("test_technique", "").lower()]) == 0:
            gaps.append("è¾¹ç•Œæµ‹è¯•è¦†ç›–ä¸è¶³")

        return gaps

    def _generate_automation_recommendations(self, test_cases: List[TestCaseData]) -> List[str]:
        """ç”Ÿæˆè‡ªåŠ¨åŒ–å»ºè®®"""
        recommendations = []

        high_automation_count = len([tc for tc in test_cases if tc.source_metadata.get("automation_feasibility") == "high"])
        total_count = len(test_cases)

        if high_automation_count / total_count > 0.7:
            recommendations.append("å»ºè®®ä¼˜å…ˆå®ç°é«˜è‡ªåŠ¨åŒ–å¯è¡Œæ€§çš„æµ‹è¯•ç”¨ä¾‹")

        if any(tc.test_type == TestType.PERFORMANCE for tc in test_cases):
            recommendations.append("æ€§èƒ½æµ‹è¯•å»ºè®®ä½¿ç”¨JMeteræˆ–LoadRunner")

        if any(tc.test_type == TestType.FUNCTIONAL for tc in test_cases):
            recommendations.append("åŠŸèƒ½æµ‹è¯•å»ºè®®ä½¿ç”¨Seleniumæˆ–Cypress")

        if any(tc.test_type == TestType.INTERFACE for tc in test_cases):
            recommendations.append("æ¥å£æµ‹è¯•å»ºè®®ä½¿ç”¨Postmanæˆ–RestAssured")

        return recommendations

    def _map_non_functional_test_type(self, nfr_type: str) -> TestType:
        """æ˜ å°„éåŠŸèƒ½éœ€æ±‚ç±»å‹åˆ°æµ‹è¯•ç±»å‹"""
        mapping = {
            "performance": TestType.PERFORMANCE,
            "security": TestType.SECURITY,
            "usability": TestType.USABILITY,
            "reliability": TestType.FUNCTIONAL,
            "scalability": TestType.PERFORMANCE,
            "compatibility": TestType.COMPATIBILITY,
            "maintainability": TestType.FUNCTIONAL,
            "availability": TestType.FUNCTIONAL
        }
        return mapping.get(nfr_type.lower(), TestType.FUNCTIONAL)

    def _map_priority(self, priority_str: str) -> Priority:
        """æ˜ å°„ä¼˜å…ˆçº§"""
        mapping = {
            "high": Priority.P1,
            "medium": Priority.P2,
            "low": Priority.P3,
            "critical": Priority.P0,
            "P0": Priority.P0,
            "P1": Priority.P1,
            "P2": Priority.P2,
            "P3": Priority.P3,
            "P4": Priority.P4
        }
        return mapping.get(priority_str, Priority.P2)

    # ==================== ä¼ä¸šçº§å“åº”å¤„ç†æ–¹æ³• ====================

    async def _sendsave_request(
        self,
        message: TestPointExtractionResponse,
        generation_result: TestCaseGenerationResult
    ) -> TestCaseSaveResponse:
        """å‘é€ä¼ä¸šçº§ä¿å­˜è¯·æ±‚åˆ°æ•°æ®åº“æ™ºèƒ½ä½“"""
        try:
            # æ„å»ºä¿å­˜è¯·æ±‚
            save_request = TestCaseSaveRequest(
                session_id=message.session_id,
                test_cases=generation_result.generated_test_cases,
                project_id=None,  # å¯ä»¥ä»æ¶ˆæ¯ä¸­è·å–
                created_by="test_point_extractor",
                source_metadata={
                    "source_type": "test_point_extraction",
                    "extraction_id": message.extraction_id,
                    "generation_strategy": generation_result.generation_strategy,
                    "quality_metrics": generation_result.quality_metrics,
                    "coverage_analysis": generation_result.coverage_analysis,
                    "automation_analysis": generation_result.automation_analysis,
                    "test_execution_plan": generation_result.test_execution_plan,
                    "test_case_categories": generation_result.test_case_categories
                }
            )

            logger.info(f"å·²å‘é€ä¼ä¸šçº§ä¿å­˜è¯·æ±‚åˆ°æ•°æ®åº“æ™ºèƒ½ä½“: {message.session_id}")

            test_case_save_response = await self.send_message(
                save_request,
                AgentId(type=TopicTypes.TEST_CASE_SAVER.value, key=self.id.key)
            )
            return test_case_save_response

        except Exception as e:
            logger.error(f"å‘é€ä¼ä¸šçº§ä¿å­˜è¯·æ±‚å¤±è´¥: {str(e)}")
            return TestCaseSaveResponse(
                session_id=message.session_id,
                success=False,
                errors=[str(e)]
            )

    async def _sendmind_map_request(
        self,
        message: TestPointExtractionResponse,
        saved_test_cases: List[Dict[str, Any]]
    ):
        """å‘é€ä¼ä¸šçº§æ€ç»´å¯¼å›¾ç”Ÿæˆè¯·æ±‚"""
        try:
            await self.send_response("ğŸ§  æ­£åœ¨ç”Ÿæˆä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹æ€ç»´å¯¼å›¾...")

            # æ„å»ºæ€ç»´å¯¼å›¾ç”Ÿæˆè¯·æ±‚
            mind_map_request = MindMapGenerationRequest(
                session_id=message.session_id,
                test_case_ids=[tc["id"] for tc in saved_test_cases],
                source_data={
                    "extraction_result": message.extraction_result,
                    "test_coverage_analysis": message.test_coverage_analysis,
                    "test_priority_matrix": message.test_priority_matrix
                },
                generation_config={
                    "layout_type": "enterprise",
                    "include_test_points": True,
                    "include_coverage_analysis": True,
                    "include_priority_matrix": True,
                    "group_by_category": True
                }
            )

            # å‘é€åˆ°æ€ç»´å¯¼å›¾ç”Ÿæˆæ™ºèƒ½ä½“
            await self.publish_message(
                mind_map_request,
                topic_id=TopicId(type=TopicTypes.MIND_MAP_GENERATOR.value, source=self.id.key)
            )

            logger.info(f"å·²å‘é€ä¼ä¸šçº§æ€ç»´å¯¼å›¾ç”Ÿæˆè¯·æ±‚: {message.session_id}")

        except Exception as e:
            logger.error(f"å‘é€ä¼ä¸šçº§æ€ç»´å¯¼å›¾ç”Ÿæˆè¯·æ±‚å¤±è´¥: {str(e)}")

    async def _handle_emptygeneration(
        self,
        message: TestPointExtractionResponse,
        generation_result: TestCaseGenerationResult
    ):
        """å¤„ç†ç©ºçš„ä¼ä¸šçº§ç”Ÿæˆç»“æœ"""
        response = TestCaseGenerationResponse(
            session_id=message.session_id,
            generation_id=str(uuid.uuid4()),
            source_type="test_point_extraction",
            generated_count=0,
            test_case_ids=[],
            mind_map_generated=False,
            processing_time=generation_result.processing_time,
            created_at=datetime.now().isoformat()
        )

        await self.send_response(
            "âš ï¸ æœªç”Ÿæˆä»»ä½•ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹",
            is_final=True,
            result=response.model_dump()
        )

    async def _sendfinal_response(
        self,
        message: TestPointExtractionResponse,
        generation_result: TestCaseGenerationResult,
        save_result: TestCaseSaveResponse,
        mind_map_generated: bool,
        start_time: datetime
    ):
        """å‘é€ä¼ä¸šçº§æœ€ç»ˆå“åº”"""
        processing_time = (datetime.now() - start_time).total_seconds()

        response = TestCaseGenerationResponse(
            session_id=message.session_id,
            generation_id=str(uuid.uuid4()),
            source_type="test_point_extraction",
            generated_count=generation_result.generated_count,
            test_case_ids=[tc["id"] for tc in save_result.saved_test_cases] if save_result.success else [],
            mind_map_generated=mind_map_generated,
            processing_time=processing_time,
            created_at=datetime.now().isoformat()
        )

        if save_result.success:
            await self.send_response(
                f"ğŸ† ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹å¤„ç†å®Œæˆï¼ç”Ÿæˆ {generation_result.generated_count} ä¸ªé«˜è´¨é‡æµ‹è¯•ç”¨ä¾‹ï¼Œ"
                f"æˆåŠŸä¿å­˜ {save_result.saved_count} ä¸ªï¼Œè´¨é‡è¯„åˆ†: {generation_result.quality_metrics.get('overall_quality_score', 0.0):.2f}",
                is_final=False,
                result={
                    **response.model_dump(),
                    "quality_metrics": generation_result.quality_metrics,
                    "coverage_analysis": generation_result.coverage_analysis,
                    "automation_analysis": generation_result.automation_analysis,
                    "test_execution_plan": generation_result.test_execution_plan,
                    "test_case_categories": generation_result.test_case_categories
                }
            )
        else:
            await self.send_response(
                f"âš ï¸ ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå®Œæˆï¼Œä½†ä¿å­˜æ—¶å‡ºç°é—®é¢˜ï¼šæˆåŠŸ {save_result.saved_count} ä¸ªï¼Œå¤±è´¥ {save_result.failed_count} ä¸ª",
                is_final=True,
                result=response.model_dump()
            )

    async def _handlegeneration_error(
        self,
        message: TestPointExtractionResponse,
        error: Exception,
        start_time: datetime
    ):
        """å¤„ç†ä¼ä¸šçº§ç”Ÿæˆé”™è¯¯"""
        processing_time = (datetime.now() - start_time).total_seconds()

        error_response = TestCaseGenerationResponse(
            session_id=message.session_id,
            generation_id=str(uuid.uuid4()),
            source_type="test_point_extraction",
            generated_count=0,
            test_case_ids=[],
            mind_map_generated=False,
            processing_time=processing_time,
            created_at=datetime.now().isoformat()
        )

        await self.send_response(
            f"âŒ ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(error)}",
            is_final=True,
            result=error_response.model_dump()
        )

        logger.error(f"ä¼ä¸šçº§æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {message.session_id}, é”™è¯¯: {str(error)}")

    def _update_quality_metrics(self, generation_result: TestCaseGenerationResult):
        """æ›´æ–°è´¨é‡æŒ‡æ ‡"""
        if generation_result.quality_metrics:
            current_avg_quality = self.quality_metrics["average_quality_score"]
            total_requests = self.quality_metrics["total_requests"]
            new_quality_score = generation_result.quality_metrics.get("overall_quality_score", 0.0)

            # è®¡ç®—æ–°çš„å¹³å‡è´¨é‡åˆ†æ•°
            new_avg_quality = ((current_avg_quality * (total_requests - 1)) + new_quality_score) / total_requests
            self.quality_metrics["average_quality_score"] = new_avg_quality

        if generation_result.automation_analysis:
            automation_score = generation_result.automation_analysis.get("overall_automation_score", 0.0)
            current_avg_automation = self.quality_metrics["automation_feasibility_score"]
            total_requests = self.quality_metrics["total_requests"]

            new_avg_automation = ((current_avg_automation * (total_requests - 1)) + automation_score) / total_requests
            self.quality_metrics["automation_feasibility_score"] = new_avg_automation

        if generation_result.coverage_analysis:
            coverage_score = generation_result.coverage_analysis.get("overall_coverage_score", 0.0)
            current_avg_coverage = self.quality_metrics["coverage_completeness_score"]
            total_requests = self.quality_metrics["total_requests"]

            new_avg_coverage = ((current_avg_coverage * (total_requests - 1)) + coverage_score) / total_requests
            self.quality_metrics["coverage_completeness_score"] = new_avg_coverage

    async def _retrieve_rag_context(
        self,
        message: TestPointExtractionResponse
    ) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨RAGçŸ¥è¯†åº“æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ - å¤šç»´åº¦åˆ†å±‚æ£€ç´¢ç­–ç•¥

        é‡‡ç”¨4ä¸ªç»´åº¦çš„RAGæŸ¥è¯¢æ¥è·å–å…¨é¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        1. ä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡ - è·å–è¡Œä¸šæ ‡å‡†å’Œä¸šåŠ¡çŸ¥è¯†
        2. æµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡ - è·å–æµ‹è¯•æŠ€æœ¯å’Œæœ€ä½³å®è·µ
        3. ç›¸ä¼¼åœºæ™¯ä¸Šä¸‹æ–‡ - è·å–ç›¸ä¼¼åŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹æ¨¡æ¿
        4. è´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡ - è·å–è´¨é‡è¦æ±‚å’Œåˆè§„æ ‡å‡†
        """
        try:
            await self.send_response("ğŸ” å¼€å§‹å¤šç»´åº¦RAGä¸Šä¸‹æ–‡æ£€ç´¢...", region="progress")

            # è§£æéœ€æ±‚åˆ†æç»“æœ
            analysis_result = message.requirement_analysis_result or {}

            # æ„å»ºå¤šç»´åº¦RAGæŸ¥è¯¢
            rag_contexts = {}

            # 1. ä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡æŸ¥è¯¢
            domain_context = await self._retrieve_domain_context(analysis_result, message)
            if domain_context:
                rag_contexts["domain"] = domain_context
                await self.send_response("âœ… ä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ", region="info")

            # 2. æµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡æŸ¥è¯¢
            methodology_context = await self._retrieve_methodology_context(message)
            if methodology_context:
                rag_contexts["methodology"] = methodology_context
                await self.send_response("âœ… æµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ", region="info")

            # 3. ç›¸ä¼¼åœºæ™¯ä¸Šä¸‹æ–‡æŸ¥è¯¢
            scenario_context = await self._retrieve_scenario_context(analysis_result, message)
            if scenario_context:
                rag_contexts["scenarios"] = scenario_context
                await self.send_response("âœ… ç›¸ä¼¼åœºæ™¯ä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ", region="info")

            # 4. è´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡æŸ¥è¯¢
            quality_context = await self._retrieve_quality_context(analysis_result, message)
            if quality_context:
                rag_contexts["quality"] = quality_context
                await self.send_response("âœ… è´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ", region="info")

            # æ±‡æ€»æ£€ç´¢ç»“æœ
            total_contexts = len(rag_contexts)
            if total_contexts > 0:
                await self.send_response(
                    f"ğŸ¯ RAGå¤šç»´åº¦æ£€ç´¢å®Œæˆ: è·å–åˆ° {total_contexts} ä¸ªç»´åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯",
                    region="success"
                )
                return rag_contexts
            else:
                await self.send_response("âš ï¸ æœªè·å–åˆ°RAGä¸Šä¸‹æ–‡ä¿¡æ¯", region="warning")
                return None

        except Exception as e:
            logger.error(f"RAGå¤šç»´åº¦ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}")
            await self.send_response(
                f"âŒ RAGä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}",
                region="error"
            )
            return None

    async def _retrieve_domain_context(
        self,
        analysis_result: Dict[str, Any],
        message: TestPointExtractionResponse
    ) -> Optional[RagRetrievalResponse]:
        """æ£€ç´¢ä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡"""
        try:
            # æ„å»ºä¸šåŠ¡é¢†åŸŸæŸ¥è¯¢
            domain_query_parts = []

            # ä»éœ€æ±‚åˆ†æä¸­æå–ä¸šåŠ¡é¢†åŸŸä¿¡æ¯
            if analysis_result.get("document_title"):
                domain_query_parts.append(analysis_result["document_title"])

            if analysis_result.get("executive_summary"):
                # æå–å…³é”®ä¸šåŠ¡è¯æ±‡
                summary = analysis_result["executive_summary"][:200]
                domain_query_parts.append(summary)

            # æ·»åŠ ä¸šåŠ¡æµç¨‹ä¿¡æ¯
            business_processes = analysis_result.get("business_processes", [])
            if business_processes:
                for process in business_processes[:2]:
                    if process.get("name"):
                        domain_query_parts.append(process["name"])

            # æ„å»ºé¢†åŸŸæŸ¥è¯¢
            domain_query = " ".join(domain_query_parts) + " è¡Œä¸šæ ‡å‡† æµ‹è¯•è§„èŒƒ ä¸šåŠ¡è§„åˆ™"

            # å‘é€RAGæ£€ç´¢è¯·æ±‚
            rag_request = RagRetrievalRequest(
                session_id=message.session_id,
                query=domain_query,
                requirements=analysis_result.get("executive_summary", ""),
                search_mode="advanced",
                search_settings={
                    "use_semantic_search": True,
                    "use_fulltext_search": True,
                    "limit": 5,
                    "filters": {
                        "metadata.context_type": {"$in": ["domain_knowledge", "industry_standards", "business_rules"]}
                    }
                },
                rag_generation_config={
                    "stream": False,
                    "max_tokens": 400
                },
                context_type="domain_knowledge",
                max_results=5
            )

            return await self._send_rag_request(rag_request)

        except Exception as e:
            logger.error(f"ä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return None

    async def _retrieve_methodology_context(
        self,
        message: TestPointExtractionResponse
    ) -> Optional[RagRetrievalResponse]:
        """æ£€ç´¢æµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡"""
        try:
            # æ„å»ºæµ‹è¯•æ–¹æ³•è®ºæŸ¥è¯¢
            methodology_query_parts = []

            # åˆ†ææµ‹è¯•ç‚¹ç±»å‹
            test_types = []
            if message.functional_test_points:
                test_types.append("åŠŸèƒ½æµ‹è¯•")
            if message.non_functional_test_points:
                test_types.append("éåŠŸèƒ½æµ‹è¯•")
            if message.integration_test_points:
                test_types.append("é›†æˆæµ‹è¯•")
            if message.boundary_test_points:
                test_types.append("è¾¹ç•Œæµ‹è¯•")
            if message.exception_test_points:
                test_types.append("å¼‚å¸¸æµ‹è¯•")

            methodology_query_parts.extend(test_types)

            # æ·»åŠ æµ‹è¯•æŠ€æœ¯å…³é”®è¯
            methodology_query_parts.extend([
                "æµ‹è¯•ç”¨ä¾‹è®¾è®¡", "æµ‹è¯•æ–¹æ³•", "æµ‹è¯•æŠ€æœ¯", "ç­‰ä»·ç±»åˆ’åˆ†",
                "è¾¹ç•Œå€¼åˆ†æ", "å†³ç­–è¡¨æµ‹è¯•", "çŠ¶æ€è½¬æ¢æµ‹è¯•"
            ])

            methodology_query = " ".join(methodology_query_parts)

            # å‘é€RAGæ£€ç´¢è¯·æ±‚
            rag_request = RagRetrievalRequest(
                session_id=message.session_id,
                query=methodology_query,
                test_points=message.functional_test_points[:3] + message.non_functional_test_points[:2],
                search_mode="advanced",
                search_settings={
                    "use_semantic_search": True,
                    "limit": 6,
                    "filters": {
                        "metadata.context_type": {"$in": ["test_methodology", "test_techniques", "best_practices"]}
                    }
                },
                rag_generation_config={
                    "stream": False,
                    "max_tokens": 500
                },
                context_type="test_methodology",
                max_results=6
            )

            return await self._send_rag_request(rag_request)

        except Exception as e:
            logger.error(f"æµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return None

    async def _retrieve_scenario_context(
        self,
        analysis_result: Dict[str, Any],
        message: TestPointExtractionResponse
    ) -> Optional[RagRetrievalResponse]:
        """æ£€ç´¢ç›¸ä¼¼åœºæ™¯ä¸Šä¸‹æ–‡"""
        try:
            # æ„å»ºç›¸ä¼¼åœºæ™¯æŸ¥è¯¢
            scenario_query_parts = []

            # æå–åŠŸèƒ½éœ€æ±‚ä½œä¸ºåœºæ™¯æè¿°
            functional_reqs = analysis_result.get("functional_requirements", [])
            if functional_reqs:
                for req in functional_reqs[:3]:
                    if req.get("title"):
                        scenario_query_parts.append(req["title"])
                    if req.get("description"):
                        scenario_query_parts.append(req["description"][:100])

            # æå–ç”¨æˆ·æ•…äº‹
            user_stories = analysis_result.get("user_stories", [])
            if user_stories:
                for story in user_stories[:2]:
                    if story.get("story"):
                        scenario_query_parts.append(story["story"][:100])

            # æ·»åŠ åœºæ™¯å…³é”®è¯
            scenario_query_parts.extend([
                "æµ‹è¯•ç”¨ä¾‹æ¨¡æ¿", "æµ‹è¯•åœºæ™¯", "ç”¨ä¾‹è®¾è®¡", "æµ‹è¯•æ­¥éª¤"
            ])

            scenario_query = " ".join(scenario_query_parts)

            # å‘é€RAGæ£€ç´¢è¯·æ±‚
            rag_request = RagRetrievalRequest(
                session_id=message.session_id,
                query=scenario_query,
                requirements=analysis_result.get("executive_summary", ""),
                test_points=message.functional_test_points[:2],
                search_mode="advanced",
                search_settings={
                    "use_semantic_search": True,
                    "limit": 5,
                    "filters": {
                        "metadata.context_type": {"$in": ["test_scenarios", "test_templates", "similar_cases"]}
                    }
                },
                rag_generation_config={
                    "stream": False,
                    "max_tokens": 400
                },
                context_type="test_scenarios",
                max_results=5
            )

            return await self._send_rag_request(rag_request)

        except Exception as e:
            logger.error(f"ç›¸ä¼¼åœºæ™¯ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return None

    async def _retrieve_quality_context(
        self,
        analysis_result: Dict[str, Any],
        message: TestPointExtractionResponse
    ) -> Optional[RagRetrievalResponse]:
        """æ£€ç´¢è´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡"""
        try:
            # æ„å»ºè´¨é‡æ ‡å‡†æŸ¥è¯¢
            quality_query_parts = []

            # æå–éåŠŸèƒ½éœ€æ±‚
            non_functional_reqs = analysis_result.get("non_functional_requirements", [])
            if non_functional_reqs:
                for req in non_functional_reqs[:3]:
                    if req.get("title"):
                        quality_query_parts.append(req["title"])

            # æå–çº¦æŸæ¡ä»¶
            constraints = analysis_result.get("constraints", [])
            if constraints:
                for constraint in constraints[:2]:
                    if constraint.get("description"):
                        quality_query_parts.append(constraint["description"][:100])

            # æ·»åŠ è´¨é‡å…³é”®è¯
            quality_query_parts.extend([
                "è´¨é‡æ ‡å‡†", "æµ‹è¯•è¦†ç›–åº¦", "éªŒæ”¶æ ‡å‡†", "åˆè§„è¦æ±‚",
                "æ€§èƒ½æ ‡å‡†", "å®‰å…¨æ ‡å‡†", "å¯ç”¨æ€§æ ‡å‡†"
            ])

            quality_query = " ".join(quality_query_parts)

            # å‘é€RAGæ£€ç´¢è¯·æ±‚
            rag_request = RagRetrievalRequest(
                session_id=message.session_id,
                query=quality_query,
                test_points=message.non_functional_test_points[:3],
                search_mode="advanced",
                search_settings={
                    "use_semantic_search": True,
                    "limit": 4,
                    "filters": {
                        "metadata.context_type": {"$in": ["quality_standards", "compliance", "acceptance_criteria"]}
                    }
                },
                rag_generation_config={
                    "stream": False,
                    "max_tokens": 300
                },
                context_type="quality_standards",
                max_results=4
            )

            return await self._send_rag_request(rag_request)

        except Exception as e:
            logger.error(f"è´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡æ£€ç´¢å¤±è´¥: {str(e)}")
            return None

    async def _send_rag_request(self, rag_request: RagRetrievalRequest) -> Optional[RagRetrievalResponse]:
        """å‘é€RAGæ£€ç´¢è¯·æ±‚"""
        try:
            # å‘é€åˆ°RAGæ£€ç´¢æ™ºèƒ½ä½“
            await self.publish_message(
                rag_request,
                topic_id=TopicId(type=TopicTypes.RAG_RETRIEVAL.value, source=self.id.key)
            )

            # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ç­‰å¾…å“åº”ï¼Œä½†åœ¨å½“å‰æ¶æ„ä¸­ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡æ¶ˆæ¯å¤„ç†å™¨æ¥æ¥æ”¶å“åº”
            # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œè¿”å›Noneï¼Œå®é™…çš„RAGå“åº”ä¼šé€šè¿‡æ¶ˆæ¯å¤„ç†å™¨å¤„ç†
            logger.info("RAGæ£€ç´¢è¯·æ±‚å·²å‘é€")
            return None

        except Exception as e:
            logger.error(f"RAGè¯·æ±‚å‘é€å¤±è´¥: {str(e)}")
            return None

    def _parse_rag_context(self, rag_contexts: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æRAGä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæå–å…³é”®ä¿¡æ¯ç”¨äºæµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ

        Args:
            rag_contexts: å¤šç»´åº¦RAGæ£€ç´¢ç»“æœ

        Returns:
            è§£æåçš„ä¸Šä¸‹æ–‡ä¿¡æ¯å­—å…¸
        """
        try:
            enhanced_context = {
                "domain_knowledge": "",
                "test_methodologies": "",
                "scenario_templates": "",
                "quality_standards": "",
                "best_practices": [],
                "industry_standards": [],
                "test_techniques": [],
                "compliance_requirements": []
            }

            # è§£æä¸šåŠ¡é¢†åŸŸä¸Šä¸‹æ–‡
            if "domain" in rag_contexts:
                domain_context = rag_contexts["domain"]
                if hasattr(domain_context, 'rag_completion') and domain_context.rag_completion:
                    enhanced_context["domain_knowledge"] = domain_context.rag_completion

                # æå–è¡Œä¸šæ ‡å‡†
                if hasattr(domain_context, 'search_results'):
                    for result in domain_context.search_results[:3]:
                        if isinstance(result, dict) and result.get("text"):
                            if "æ ‡å‡†" in result["text"] or "è§„èŒƒ" in result["text"]:
                                enhanced_context["industry_standards"].append(result["text"][:200])

            # è§£ææµ‹è¯•æ–¹æ³•è®ºä¸Šä¸‹æ–‡
            if "methodology" in rag_contexts:
                methodology_context = rag_contexts["methodology"]
                if hasattr(methodology_context, 'rag_completion') and methodology_context.rag_completion:
                    enhanced_context["test_methodologies"] = methodology_context.rag_completion

                # æå–æµ‹è¯•æŠ€æœ¯
                if hasattr(methodology_context, 'search_results'):
                    for result in methodology_context.search_results[:3]:
                        if isinstance(result, dict) and result.get("text"):
                            if any(tech in result["text"] for tech in ["ç­‰ä»·ç±»", "è¾¹ç•Œå€¼", "å†³ç­–è¡¨", "çŠ¶æ€è½¬æ¢"]):
                                enhanced_context["test_techniques"].append(result["text"][:200])

            # è§£æåœºæ™¯æ¨¡æ¿ä¸Šä¸‹æ–‡
            if "scenarios" in rag_contexts:
                scenario_context = rag_contexts["scenarios"]
                if hasattr(scenario_context, 'rag_completion') and scenario_context.rag_completion:
                    enhanced_context["scenario_templates"] = scenario_context.rag_completion

                # æå–æœ€ä½³å®è·µ
                if hasattr(scenario_context, 'search_results'):
                    for result in scenario_context.search_results[:3]:
                        if isinstance(result, dict) and result.get("text"):
                            if "æœ€ä½³å®è·µ" in result["text"] or "æ¨¡æ¿" in result["text"]:
                                enhanced_context["best_practices"].append(result["text"][:200])

            # è§£æè´¨é‡æ ‡å‡†ä¸Šä¸‹æ–‡
            if "quality" in rag_contexts:
                quality_context = rag_contexts["quality"]
                if hasattr(quality_context, 'rag_completion') and quality_context.rag_completion:
                    enhanced_context["quality_standards"] = quality_context.rag_completion

                # æå–åˆè§„è¦æ±‚
                if hasattr(quality_context, 'search_results'):
                    for result in quality_context.search_results[:3]:
                        if isinstance(result, dict) and result.get("text"):
                            if "åˆè§„" in result["text"] or "æ ‡å‡†" in result["text"]:
                                enhanced_context["compliance_requirements"].append(result["text"][:200])

            logger.info(f"RAGä¸Šä¸‹æ–‡è§£æå®Œæˆ: è·å–åˆ° {len([k for k, v in enhanced_context.items() if v])} ä¸ªæœ‰æ•ˆä¸Šä¸‹æ–‡")
            return enhanced_context

        except Exception as e:
            logger.error(f"RAGä¸Šä¸‹æ–‡è§£æå¤±è´¥: {str(e)}")
            return {}
