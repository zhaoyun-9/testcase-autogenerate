"""
æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“
ä¸“é—¨è´Ÿè´£ä»éœ€æ±‚è§£æç»“æœä¸­æå–ä¸“ä¸šçš„æµ‹è¯•ç‚¹ï¼Œè¿›è¡Œä¼ä¸šçº§æµ‹è¯•è¦†ç›–åº¦åˆ†æ
åŸºäºAutoGen Coreæ¶æ„å®ç°ï¼Œå‚è€ƒrequirement_analysis_agent.pyçš„ä¼˜ç§€è®¾è®¡æ¨¡å¼
"""
import uuid
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime

from autogen_agentchat.base import TaskResult
from autogen_agentchat.messages import ModelClientStreamingChunkEvent
from autogen_core import message_handler, type_subscription, MessageContext, TopicId
from loguru import logger
from pydantic import BaseModel, Field

from app.core.agents.base import BaseAgent
from app.core.types import TopicTypes, AgentTypes, AGENT_NAMES
from app.core.messages.test_case import (
    TestPointExtractionRequest, TestPointExtractionResponse,
    TestCaseData
)
from app.core.enums import TestType, TestLevel, Priority, InputSource


class TestPointExtractionResult(BaseModel):
    """æµ‹è¯•ç‚¹æå–ç»“æœ"""
    extraction_strategy: str = Field(..., description="æå–ç­–ç•¥")
    coverage_analysis: Dict[str, Any] = Field(default_factory=dict, description="è¦†ç›–åº¦åˆ†æ")
    functional_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="åŠŸèƒ½æµ‹è¯•ç‚¹")
    non_functional_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="éåŠŸèƒ½æµ‹è¯•ç‚¹")
    integration_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="é›†æˆæµ‹è¯•ç‚¹")
    acceptance_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="éªŒæ”¶æµ‹è¯•ç‚¹")
    boundary_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="è¾¹ç•Œæµ‹è¯•ç‚¹")
    exception_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="å¼‚å¸¸æµ‹è¯•ç‚¹")
    security_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="å®‰å…¨æµ‹è¯•ç‚¹")
    performance_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="æ€§èƒ½æµ‹è¯•ç‚¹")
    usability_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="å¯ç”¨æ€§æµ‹è¯•ç‚¹")
    compatibility_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="å…¼å®¹æ€§æµ‹è¯•ç‚¹")
    test_data_requirements: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•æ•°æ®éœ€æ±‚")
    test_environment_requirements: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•ç¯å¢ƒéœ€æ±‚")
    test_dependency_matrix: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•ä¾èµ–çŸ©é˜µ")
    test_priority_matrix: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•ä¼˜å…ˆçº§çŸ©é˜µ")
    risk_based_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="åŸºäºé£é™©çš„æµ‹è¯•ç‚¹")
    regression_test_points: List[Dict[str, Any]] = Field(default_factory=list, description="å›å½’æµ‹è¯•ç‚¹")
    automation_feasibility: Dict[str, Any] = Field(default_factory=dict, description="è‡ªåŠ¨åŒ–å¯è¡Œæ€§åˆ†æ")
    test_execution_sequence: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•æ‰§è¡Œåºåˆ—")
    confidence_score: float = Field(0.0, description="æå–ç½®ä¿¡åº¦")


@type_subscription(topic_type=TopicTypes.TEST_POINT_EXTRACTOR.value)
class TestPointExtractionAgent(BaseAgent):
    """æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“ï¼Œä¸“é—¨è´Ÿè´£ä¼ä¸šçº§ä¸“ä¸šçš„æµ‹è¯•ç‚¹æå–å’Œåˆ†æ"""

    def __init__(self, model_client_instance=None, **kwargs):
        """åˆå§‹åŒ–æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“"""
        super().__init__(
            agent_id=AgentTypes.TEST_POINT_EXTRACTOR.value,
            agent_name=AGENT_NAMES.get(AgentTypes.TEST_POINT_EXTRACTOR.value, "æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“"),
            model_client_instance=model_client_instance,
            **kwargs
        )
        
        # æµ‹è¯•ç‚¹æå–é…ç½®
        self.extraction_config = {
            'enable_functional_extraction': True,
            'enable_non_functional_extraction': True,
            'enable_integration_extraction': True,
            'enable_boundary_extraction': True,
            'enable_exception_extraction': True,
            'enable_security_extraction': True,
            'enable_performance_extraction': True,
            'enable_usability_extraction': True,
            'enable_compatibility_extraction': True,
            'enable_risk_based_extraction': True,
            'enable_automation_analysis': True,
            'confidence_threshold': 0.75,
            'max_test_points_per_category': 100,
            'priority_levels': ['P0', 'P1', 'P2', 'P3', 'P4'],
            'test_techniques': [
                'equivalence_partitioning',
                'boundary_value_analysis',
                'decision_table_testing',
                'state_transition_testing',
                'use_case_testing',
                'exploratory_testing'
            ]
        }
        
        logger.info(f"æµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ: {self.agent_name}")

    @message_handler
    async def handle_test_point_extraction_request(
        self,
        message: TestPointExtractionRequest,
        ctx: MessageContext
    ) -> None:
        """å¤„ç†æµ‹è¯•ç‚¹æå–è¯·æ±‚"""
        start_time = datetime.now()

        try:
            logger.info(f"å¼€å§‹å¤„ç†æµ‹è¯•ç‚¹æå–è¯·æ±‚: {message.session_id}")

            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            await self.send_response(
                f"ğŸ¯ å¼€å§‹ä¼ä¸šçº§æµ‹è¯•ç‚¹æå–: åŸºäºéœ€æ±‚è§£æç»“æœ",
                region="process"
            )

            # åˆ†æéœ€æ±‚è§£æç»“æœ
            analysis_result = message.requirement_analysis_result
            requirements_count = len(analysis_result.get('requirements', []))
            business_processes_count = len(analysis_result.get('business_processes', []))
            
            await self.send_response(
                f"ğŸ“Š éœ€æ±‚åˆ†æè¾“å…¥: {requirements_count} ä¸ªéœ€æ±‚, {business_processes_count} ä¸ªä¸šåŠ¡æµç¨‹",
                region="info"
            )

            # æ‰§è¡Œæµ‹è¯•ç‚¹æå–
            await self.send_response("ğŸ”„ ç¬¬1æ­¥: å¼€å§‹ä¸“ä¸šæµ‹è¯•ç‚¹æå–åˆ†æ...", region="progress")
            extraction_result = await self._extract_test_points(message)

            # å‘é€æå–ç»“æœç»Ÿè®¡
            total_test_points = (
                len(extraction_result.functional_test_points) +
                len(extraction_result.non_functional_test_points) +
                len(extraction_result.integration_test_points) +
                len(extraction_result.acceptance_test_points) +
                len(extraction_result.boundary_test_points) +
                len(extraction_result.exception_test_points)
            )

            await self.send_response(
                f"ğŸ“ˆ æµ‹è¯•ç‚¹æå–å®Œæˆ: åŠŸèƒ½æµ‹è¯•ç‚¹ {len(extraction_result.functional_test_points)} ä¸ª, "
                f"éåŠŸèƒ½æµ‹è¯•ç‚¹ {len(extraction_result.non_functional_test_points)} ä¸ª, "
                f"é›†æˆæµ‹è¯•ç‚¹ {len(extraction_result.integration_test_points)} ä¸ª, "
                f"æ€»è®¡ {total_test_points} ä¸ªæµ‹è¯•ç‚¹",
                region="info",
                result={
                    "functional_test_points_count": len(extraction_result.functional_test_points),
                    "non_functional_test_points_count": len(extraction_result.non_functional_test_points),
                    "integration_test_points_count": len(extraction_result.integration_test_points),
                    "acceptance_test_points_count": len(extraction_result.acceptance_test_points),
                    "boundary_test_points_count": len(extraction_result.boundary_test_points),
                    "exception_test_points_count": len(extraction_result.exception_test_points),
                    "total_test_points": total_test_points,
                    "confidence_score": extraction_result.confidence_score
                }
            )

            # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            await self.send_response("ğŸ”„ ç¬¬2æ­¥: åŸºäºæµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...", region="progress")
            test_cases = await self._generate_test_cases_from_test_points(
                extraction_result, message
            )

            # å‘é€æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ
            await self.send_response(
                f"âœ… æˆåŠŸç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹",
                region="success",
                result={"test_cases_count": len(test_cases)}
            )

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()

            # æ„å»ºå“åº”
            response = TestPointExtractionResponse(
                session_id=message.session_id,
                extraction_id=str(uuid.uuid4()),
                requirement_analysis_result=message.requirement_analysis_result,
                extraction_result=extraction_result.model_dump(),
                functional_test_points=extraction_result.functional_test_points,
                non_functional_test_points=extraction_result.non_functional_test_points,
                integration_test_points=extraction_result.integration_test_points,
                acceptance_test_points=extraction_result.acceptance_test_points,
                boundary_test_points=extraction_result.boundary_test_points,
                exception_test_points=extraction_result.exception_test_points,
                test_coverage_analysis=extraction_result.coverage_analysis,
                test_priority_matrix=extraction_result.test_priority_matrix,
                processing_time=processing_time,
                created_at=datetime.now().isoformat()
            )

            # å‘é€å®Œæˆæ¶ˆæ¯
            await self.send_response(
                f"âœ… æµ‹è¯•ç‚¹æå–å®Œæˆ! å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’",
                is_final=False,
                region="success",
                result={
                    "processing_time": processing_time,
                    "total_test_points": total_test_points,
                    "coverage_score": extraction_result.coverage_analysis.get("overall_coverage", 0.0),
                    "confidence_score": extraction_result.confidence_score
                }
            )

            # å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“
            await self.send_response("ğŸ”„ è½¬å‘åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“è¿›è¡Œç”¨ä¾‹ç”Ÿæˆ...", region="info")
            await self._send_to_test_case_generator(response, test_cases)

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"æµ‹è¯•ç‚¹æå–å¤±è´¥: {str(e)}")
            await self.send_response(
                f"âŒ æµ‹è¯•ç‚¹æå–å¤±è´¥: {str(e)} (å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’)",
                is_final=True,
                region="error",
                result={"processing_time": processing_time, "error": str(e)}
            )

    async def _extract_test_points(
        self,
        message: TestPointExtractionRequest
    ) -> TestPointExtractionResult:
        """æå–æµ‹è¯•ç‚¹"""
        try:
            # åˆ›å»ºAIåˆ†ææ™ºèƒ½ä½“
            agent = self._create_test_point_extraction_agent()

            # æ„å»ºæå–æç¤º
            extraction_prompt = self._build_test_point_extraction_prompt(message)

            # æ‰§è¡ŒAIåˆ†æ
            extraction_result = await self._run_ai_extraction(agent, extraction_prompt)

            # è§£æAIå“åº”
            return self._parse_ai_extraction_result(extraction_result, message)

        except Exception as e:
            logger.error(f"AIæµ‹è¯•ç‚¹æå–å¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€æå–ç»“æœ
            return TestPointExtractionResult(
                extraction_strategy="basic",
                coverage_analysis={"overall_coverage": 0.5, "analysis_status": "failed"},
                functional_test_points=[],
                non_functional_test_points=[],
                integration_test_points=[],
                acceptance_test_points=[],
                boundary_test_points=[],
                exception_test_points=[],
                security_test_points=[],
                performance_test_points=[],
                usability_test_points=[],
                compatibility_test_points=[],
                test_data_requirements=[],
                test_environment_requirements=[],
                test_dependency_matrix=[],
                test_priority_matrix=[],
                risk_based_test_points=[],
                regression_test_points=[],
                automation_feasibility={"automation_score": 0.3, "analysis_status": "failed"},
                test_execution_sequence=[],
                confidence_score=0.3
            )

    def _create_test_point_extraction_agent(self):
        """åˆ›å»ºæµ‹è¯•ç‚¹æå–æ™ºèƒ½ä½“"""
        from app.agents.factory import agent_factory

        return agent_factory.create_assistant_agent(
            name="test_point_extractor",
            system_message=self._build_test_point_extraction_system_prompt(),
            model_client_type="deepseek"
        )

    def _build_test_point_extraction_system_prompt(self) -> str:
        """æ„å»ºæµ‹è¯•ç‚¹æå–ç³»ç»Ÿæç¤º"""
        return """
ä½ æ˜¯èµ„æ·±çš„ä¼ä¸šçº§æµ‹è¯•å·¥ç¨‹ä¸“å®¶ï¼Œæ‹¥æœ‰ä¸°å¯Œçš„æµ‹è¯•è®¾è®¡å’Œæµ‹è¯•ç­–ç•¥åˆ¶å®šç»éªŒï¼Œæ“…é•¿ä»éœ€æ±‚åˆ†æç»“æœä¸­æå–ä¸“ä¸šçš„æµ‹è¯•ç‚¹ã€‚

ä½ çš„ä¸“ä¸šèƒ½åŠ›åŒ…æ‹¬ï¼š
1. æµ‹è¯•è¦†ç›–åº¦åˆ†æå’Œæµ‹è¯•ç‚¹è¯†åˆ«
2. æµ‹è¯•æŠ€æœ¯åº”ç”¨ï¼ˆç­‰ä»·ç±»åˆ’åˆ†ã€è¾¹ç•Œå€¼åˆ†æã€å†³ç­–è¡¨æµ‹è¯•ç­‰ï¼‰
3. æµ‹è¯•ä¼˜å…ˆçº§è¯„ä¼°å’Œé£é™©åˆ†æ
4. æµ‹è¯•è‡ªåŠ¨åŒ–å¯è¡Œæ€§åˆ†æ
5. æµ‹è¯•æ•°æ®å’Œç¯å¢ƒéœ€æ±‚åˆ†æ
6. æµ‹è¯•æ‰§è¡Œç­–ç•¥åˆ¶å®š

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ä¼ä¸šçº§æµ‹è¯•ç‚¹æå–ç»“æœï¼š
{
    "extraction_strategy": "comprehensive",
    "coverage_analysis": {
        "overall_coverage": 0.95,
        "functional_coverage": 0.98,
        "non_functional_coverage": 0.90,
        "integration_coverage": 0.85,
        "boundary_coverage": 0.92,
        "exception_coverage": 0.88,
        "coverage_gaps": ["ç‰¹å®šè¾¹ç•Œæ¡ä»¶", "å¹¶å‘åœºæ™¯"],
        "coverage_recommendations": ["å¢åŠ å‹åŠ›æµ‹è¯•", "å®Œå–„å¼‚å¸¸å¤„ç†æµ‹è¯•"]
    },
    "functional_test_points": [
        {
            "id": "FTP-001",
            "name": "ç”¨æˆ·ç™»å½•åŠŸèƒ½æµ‹è¯•",
            "description": "éªŒè¯ç”¨æˆ·ç™»å½•åŠŸèƒ½çš„æ­£ç¡®æ€§",
            "category": "æ ¸å¿ƒåŠŸèƒ½",
            "priority": "P0",
            "test_technique": "equivalence_partitioning",
            "preconditions": ["ç”¨æˆ·è´¦æˆ·å­˜åœ¨", "ç³»ç»Ÿæ­£å¸¸è¿è¡Œ"],
            "test_scenarios": [
                "æœ‰æ•ˆç”¨æˆ·åå’Œå¯†ç ç™»å½•",
                "æ— æ•ˆç”¨æˆ·åç™»å½•",
                "æ— æ•ˆå¯†ç ç™»å½•"
            ],
            "expected_results": ["ç™»å½•æˆåŠŸ", "æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯"],
            "test_data_requirements": ["æœ‰æ•ˆç”¨æˆ·è´¦æˆ·", "æ— æ•ˆç”¨æˆ·æ•°æ®"],
            "automation_feasibility": "high",
            "risk_level": "high",
            "business_impact": "critical",
            "related_requirements": ["REQ-001", "REQ-002"]
        }
    ],
    "non_functional_test_points": [
        {
            "id": "NFTP-001",
            "name": "ç™»å½•å“åº”æ—¶é—´æµ‹è¯•",
            "description": "éªŒè¯ç™»å½•å“åº”æ—¶é—´ä¸è¶…è¿‡2ç§’",
            "category": "æ€§èƒ½æµ‹è¯•",
            "type": "performance",
            "priority": "P1",
            "test_technique": "load_testing",
            "performance_criteria": "å“åº”æ—¶é—´ < 2ç§’",
            "load_conditions": "å¹¶å‘ç”¨æˆ·æ•°: 1000",
            "test_environment": "ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿ",
            "monitoring_metrics": ["å“åº”æ—¶é—´", "ååé‡", "é”™è¯¯ç‡"],
            "automation_feasibility": "high",
            "related_requirements": ["NFR-001"]
        }
    ],
    "integration_test_points": [
        {
            "id": "ITP-001",
            "name": "ç”¨æˆ·æœåŠ¡ä¸è®¤è¯æœåŠ¡é›†æˆæµ‹è¯•",
            "description": "éªŒè¯ç”¨æˆ·æœåŠ¡ä¸è®¤è¯æœåŠ¡çš„é›†æˆ",
            "category": "æœåŠ¡é›†æˆ",
            "priority": "P1",
            "integration_type": "service_to_service",
            "components": ["ç”¨æˆ·æœåŠ¡", "è®¤è¯æœåŠ¡"],
            "integration_scenarios": ["æ­£å¸¸è®¤è¯æµç¨‹", "è®¤è¯å¤±è´¥å¤„ç†"],
            "data_flow": "ç”¨æˆ·è¯·æ±‚ -> ç”¨æˆ·æœåŠ¡ -> è®¤è¯æœåŠ¡ -> è¿”å›ç»“æœ",
            "test_environment": "é›†æˆæµ‹è¯•ç¯å¢ƒ",
            "automation_feasibility": "medium"
        }
    ],
    "acceptance_test_points": [
        {
            "id": "ATP-001",
            "name": "ç”¨æˆ·æ³¨å†ŒéªŒæ”¶æµ‹è¯•",
            "description": "éªŒè¯ç”¨æˆ·æ³¨å†ŒåŠŸèƒ½æ»¡è¶³ä¸šåŠ¡éœ€æ±‚",
            "category": "ä¸šåŠ¡éªŒæ”¶",
            "priority": "P0",
            "user_story": "ä½œä¸ºæ–°ç”¨æˆ·ï¼Œæˆ‘å¸Œæœ›èƒ½å¤Ÿæ³¨å†Œè´¦æˆ·ï¼Œä»¥ä¾¿ä½¿ç”¨ç³»ç»ŸåŠŸèƒ½",
            "acceptance_criteria": [
                "ç”¨æˆ·å¯ä»¥é€šè¿‡é‚®ç®±æ³¨å†Œ",
                "æ³¨å†Œåè‡ªåŠ¨å‘é€éªŒè¯é‚®ä»¶",
                "éªŒè¯åè´¦æˆ·æ¿€æ´»"
            ],
            "business_scenarios": ["æ­£å¸¸æ³¨å†Œæµç¨‹", "é‡å¤é‚®ç®±æ³¨å†Œ"],
            "stakeholders": ["æœ€ç»ˆç”¨æˆ·", "äº§å“ç»ç†"],
            "automation_feasibility": "medium"
        }
    ],
    "boundary_test_points": [
        {
            "id": "BTP-001",
            "name": "å¯†ç é•¿åº¦è¾¹ç•Œæµ‹è¯•",
            "description": "æµ‹è¯•å¯†ç é•¿åº¦çš„è¾¹ç•Œæ¡ä»¶",
            "category": "è¾¹ç•Œå€¼æµ‹è¯•",
            "priority": "P2",
            "test_technique": "boundary_value_analysis",
            "boundary_conditions": [
                "æœ€å°é•¿åº¦: 8ä½",
                "æœ€å¤§é•¿åº¦: 128ä½",
                "è¾¹ç•Œå€¼: 7ä½, 8ä½, 9ä½, 127ä½, 128ä½, 129ä½"
            ],
            "test_values": ["7ä½å¯†ç ", "8ä½å¯†ç ", "128ä½å¯†ç ", "129ä½å¯†ç "],
            "expected_behaviors": ["æ‹’ç»", "æ¥å—", "æ¥å—", "æ‹’ç»"],
            "automation_feasibility": "high"
        }
    ],
    "exception_test_points": [
        {
            "id": "ETP-001",
            "name": "ç½‘ç»œå¼‚å¸¸å¤„ç†æµ‹è¯•",
            "description": "æµ‹è¯•ç½‘ç»œå¼‚å¸¸æƒ…å†µä¸‹çš„ç³»ç»Ÿè¡Œä¸º",
            "category": "å¼‚å¸¸å¤„ç†",
            "priority": "P2",
            "exception_types": ["ç½‘ç»œè¶…æ—¶", "è¿æ¥ä¸­æ–­", "æœåŠ¡ä¸å¯ç”¨"],
            "test_scenarios": [
                "ç™»å½•æ—¶ç½‘ç»œè¶…æ—¶",
                "æ•°æ®ä¼ è¾“ä¸­è¿æ¥ä¸­æ–­",
                "ä¾èµ–æœåŠ¡ä¸å¯ç”¨"
            ],
            "expected_behaviors": [
                "æ˜¾ç¤ºè¶…æ—¶é”™è¯¯ä¿¡æ¯",
                "æ•°æ®å›æ»š",
                "é™çº§å¤„ç†"
            ],
            "recovery_mechanisms": ["é‡è¯•æœºåˆ¶", "ç¼“å­˜æœºåˆ¶", "å¤‡ç”¨æœåŠ¡"],
            "automation_feasibility": "medium"
        }
    ],
    "security_test_points": [
        {
            "id": "STP-001",
            "name": "SQLæ³¨å…¥é˜²æŠ¤æµ‹è¯•",
            "description": "æµ‹è¯•ç³»ç»Ÿå¯¹SQLæ³¨å…¥æ”»å‡»çš„é˜²æŠ¤èƒ½åŠ›",
            "category": "å®‰å…¨æµ‹è¯•",
            "priority": "P1",
            "security_type": "injection_attack",
            "attack_vectors": [
                "ç™»å½•è¡¨å•SQLæ³¨å…¥",
                "æœç´¢æ¡†SQLæ³¨å…¥",
                "URLå‚æ•°SQLæ³¨å…¥"
            ],
            "test_payloads": ["' OR '1'='1", "'; DROP TABLE users; --"],
            "expected_behaviors": ["è¾“å…¥è¢«è¿‡æ»¤", "æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯", "è®°å½•å®‰å…¨æ—¥å¿—"],
            "security_controls": ["è¾“å…¥éªŒè¯", "å‚æ•°åŒ–æŸ¥è¯¢", "WAFé˜²æŠ¤"],
            "automation_feasibility": "high"
        }
    ],
    "performance_test_points": [
        {
            "id": "PTP-001",
            "name": "ç³»ç»Ÿå¹¶å‘æ€§èƒ½æµ‹è¯•",
            "description": "æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„æ€§èƒ½è¡¨ç°",
            "category": "æ€§èƒ½æµ‹è¯•",
            "priority": "P1",
            "performance_type": "load_test",
            "load_scenarios": [
                "æ­£å¸¸è´Ÿè½½: 1000å¹¶å‘ç”¨æˆ·",
                "å³°å€¼è´Ÿè½½: 5000å¹¶å‘ç”¨æˆ·",
                "å‹åŠ›æµ‹è¯•: 10000å¹¶å‘ç”¨æˆ·"
            ],
            "performance_metrics": [
                "å“åº”æ—¶é—´ < 2ç§’",
                "ååé‡ > 1000 TPS",
                "é”™è¯¯ç‡ < 0.1%"
            ],
            "test_duration": "30åˆ†é’Ÿ",
            "monitoring_tools": ["JMeter", "Grafana", "Prometheus"],
            "automation_feasibility": "high"
        }
    ],
    "usability_test_points": [
        {
            "id": "UTP-001",
            "name": "ç”¨æˆ·ç•Œé¢æ˜“ç”¨æ€§æµ‹è¯•",
            "description": "æµ‹è¯•ç”¨æˆ·ç•Œé¢çš„æ˜“ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒ",
            "category": "å¯ç”¨æ€§æµ‹è¯•",
            "priority": "P2",
            "usability_aspects": [
                "ç•Œé¢ç›´è§‚æ€§",
                "æ“ä½œä¾¿æ·æ€§",
                "é”™è¯¯æç¤ºæ¸…æ™°æ€§"
            ],
            "test_scenarios": [
                "æ–°ç”¨æˆ·é¦–æ¬¡ä½¿ç”¨",
                "å¸¸ç”¨åŠŸèƒ½æ“ä½œ",
                "é”™è¯¯æ¢å¤æ“ä½œ"
            ],
            "success_criteria": [
                "ä»»åŠ¡å®Œæˆç‡ > 90%",
                "ç”¨æˆ·æ»¡æ„åº¦ > 4.0/5.0",
                "é”™è¯¯æ¢å¤æ—¶é—´ < 30ç§’"
            ],
            "test_methods": ["ç”¨æˆ·æµ‹è¯•", "ä¸“å®¶è¯„å®¡", "A/Bæµ‹è¯•"],
            "automation_feasibility": "low"
        }
    ],
    "compatibility_test_points": [
        {
            "id": "CTP-001",
            "name": "æµè§ˆå™¨å…¼å®¹æ€§æµ‹è¯•",
            "description": "æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒæµè§ˆå™¨ä¸Šçš„å…¼å®¹æ€§",
            "category": "å…¼å®¹æ€§æµ‹è¯•",
            "priority": "P2",
            "compatibility_matrix": [
                "Chrome æœ€æ–°ç‰ˆæœ¬",
                "Firefox æœ€æ–°ç‰ˆæœ¬",
                "Safari æœ€æ–°ç‰ˆæœ¬",
                "Edge æœ€æ–°ç‰ˆæœ¬"
            ],
            "test_scenarios": [
                "åŸºæœ¬åŠŸèƒ½æ“ä½œ",
                "ç•Œé¢æ˜¾ç¤ºæ•ˆæœ",
                "JavaScriptåŠŸèƒ½"
            ],
            "expected_results": "æ‰€æœ‰æµè§ˆå™¨åŠŸèƒ½ä¸€è‡´",
            "automation_feasibility": "high"
        }
    ],
    "test_data_requirements": [
        {
            "category": "ç”¨æˆ·æ•°æ®",
            "description": "æµ‹è¯•ç”¨æˆ·è´¦æˆ·æ•°æ®",
            "data_types": ["æœ‰æ•ˆç”¨æˆ·", "æ— æ•ˆç”¨æˆ·", "ç‰¹æ®Šå­—ç¬¦ç”¨æˆ·"],
            "data_volume": "1000æ¡è®°å½•",
            "data_sources": ["æµ‹è¯•æ•°æ®åº“", "æ•°æ®ç”Ÿæˆå·¥å…·"],
            "data_privacy": "è„±æ•å¤„ç†"
        }
    ],
    "test_environment_requirements": [
        {
            "environment_type": "é›†æˆæµ‹è¯•ç¯å¢ƒ",
            "description": "ç”¨äºé›†æˆæµ‹è¯•çš„ç¯å¢ƒé…ç½®",
            "infrastructure": ["åº”ç”¨æœåŠ¡å™¨", "æ•°æ®åº“æœåŠ¡å™¨", "ç¼“å­˜æœåŠ¡å™¨"],
            "configuration": "ä¸ç”Ÿäº§ç¯å¢ƒä¸€è‡´",
            "data_setup": "æµ‹è¯•æ•°æ®åˆå§‹åŒ–",
            "monitoring": "æ€§èƒ½ç›‘æ§å·¥å…·"
        }
    ],
    "test_dependency_matrix": [
        {
            "test_point_id": "FTP-001",
            "dependencies": ["ç”¨æˆ·æ•°æ®å‡†å¤‡", "è®¤è¯æœåŠ¡å¯ç”¨"],
            "dependency_type": "data_dependency",
            "impact_level": "high"
        }
    ],
    "test_priority_matrix": [
        {
            "priority": "P0",
            "description": "å…³é”®ä¸šåŠ¡åŠŸèƒ½",
            "test_points": ["FTP-001", "ATP-001"],
            "execution_order": 1,
            "risk_level": "high"
        }
    ],
    "risk_based_test_points": [
        {
            "risk_id": "RISK-001",
            "risk_description": "ç”¨æˆ·æ•°æ®æ³„éœ²é£é™©",
            "risk_level": "high",
            "mitigation_test_points": ["STP-001", "STP-002"],
            "test_approach": "å®‰å…¨æµ‹è¯•ä¼˜å…ˆ"
        }
    ],
    "regression_test_points": [
        {
            "id": "RTP-001",
            "name": "æ ¸å¿ƒåŠŸèƒ½å›å½’æµ‹è¯•",
            "description": "éªŒè¯æ ¸å¿ƒåŠŸèƒ½åœ¨å˜æ›´åä»æ­£å¸¸å·¥ä½œ",
            "scope": "æ ¸å¿ƒä¸šåŠ¡æµç¨‹",
            "trigger_conditions": ["ä»£ç å˜æ›´", "é…ç½®å˜æ›´"],
            "test_points": ["FTP-001", "FTP-002", "FTP-003"],
            "automation_level": "full"
        }
    ],
    "automation_feasibility": {
        "overall_automation_score": 0.85,
        "high_automation": ["åŠŸèƒ½æµ‹è¯•", "æ€§èƒ½æµ‹è¯•", "å®‰å…¨æµ‹è¯•"],
        "medium_automation": ["é›†æˆæµ‹è¯•", "å…¼å®¹æ€§æµ‹è¯•"],
        "low_automation": ["å¯ç”¨æ€§æµ‹è¯•", "æ¢ç´¢æ€§æµ‹è¯•"],
        "automation_tools": ["Selenium", "JMeter", "Postman"],
        "automation_strategy": "ä¼˜å…ˆè‡ªåŠ¨åŒ–é«˜é¢‘ã€é«˜é£é™©æµ‹è¯•ç‚¹"
    },
    "test_execution_sequence": [
        {
            "phase": "å•å…ƒæµ‹è¯•é˜¶æ®µ",
            "order": 1,
            "test_types": ["åŠŸèƒ½æµ‹è¯•", "è¾¹ç•Œæµ‹è¯•"],
            "parallel_execution": true
        },
        {
            "phase": "é›†æˆæµ‹è¯•é˜¶æ®µ",
            "order": 2,
            "test_types": ["é›†æˆæµ‹è¯•", "æ¥å£æµ‹è¯•"],
            "dependencies": ["å•å…ƒæµ‹è¯•é€šè¿‡"]
        }
    ],
    "confidence_score": 0.92
}

æ³¨æ„ï¼š
- æ·±å…¥åˆ†æéœ€æ±‚è§£æç»“æœï¼Œè¯†åˆ«æ‰€æœ‰å¯æµ‹è¯•ç‚¹
- åº”ç”¨ä¸“ä¸šæµ‹è¯•æŠ€æœ¯å’Œæ–¹æ³•
- è€ƒè™‘æµ‹è¯•çš„å¯æ‰§è¡Œæ€§å’Œè‡ªåŠ¨åŒ–å¯è¡Œæ€§
- ç¡®ä¿æµ‹è¯•è¦†ç›–åº¦å’Œè´¨é‡
- è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œå»æ‰ ```json å’Œ ```
- åˆ†æè¦ä¸“ä¸šã€å…¨é¢ã€å¯æ“ä½œ
"""

    def _build_test_point_extraction_prompt(
        self,
        message: TestPointExtractionRequest
    ) -> str:
        """æ„å»ºæµ‹è¯•ç‚¹æå–æç¤º"""
        analysis_result = message.requirement_analysis_result

        return f"""
è¯·åŸºäºä»¥ä¸‹éœ€æ±‚è§£æç»“æœï¼Œè¿›è¡Œä¼ä¸šçº§ä¸“ä¸šçš„æµ‹è¯•ç‚¹æå–å’Œåˆ†æï¼š

é¡¹ç›®ID: {message.project_id or "æœªæŒ‡å®š"}
æµ‹è¯•ç­–ç•¥: {message.test_strategy or "ç»¼åˆæµ‹è¯•ç­–ç•¥"}
æå–é…ç½®: {json.dumps(message.extraction_config or {}, ensure_ascii=False, indent=2)}

éœ€æ±‚è§£æç»“æœï¼š
{json.dumps(analysis_result, ensure_ascii=False, indent=2)[:20000]}  # é™åˆ¶å†…å®¹é•¿åº¦é¿å…tokenè¶…é™

è¯·æ ¹æ®éœ€æ±‚è§£æç»“æœï¼Œè¿›è¡Œå…¨é¢çš„ä¼ä¸šçº§æµ‹è¯•ç‚¹æå–ï¼ŒåŒ…æ‹¬ï¼š

1. **åŠŸèƒ½æµ‹è¯•ç‚¹æå–**ï¼š
   - åŸºäºåŠŸèƒ½éœ€æ±‚è¯†åˆ«æµ‹è¯•ç‚¹
   - åº”ç”¨ç­‰ä»·ç±»åˆ’åˆ†å’Œè¾¹ç•Œå€¼åˆ†æ
   - è€ƒè™‘æ­£å¸¸æµç¨‹å’Œå¼‚å¸¸æµç¨‹

2. **éåŠŸèƒ½æµ‹è¯•ç‚¹æå–**ï¼š
   - æ€§èƒ½æµ‹è¯•ç‚¹ï¼ˆå“åº”æ—¶é—´ã€ååé‡ã€å¹¶å‘ï¼‰
   - å®‰å…¨æµ‹è¯•ç‚¹ï¼ˆè®¤è¯ã€æˆæƒã€æ•°æ®ä¿æŠ¤ï¼‰
   - å¯ç”¨æ€§æµ‹è¯•ç‚¹ï¼ˆæ˜“ç”¨æ€§ã€å¯è®¿é—®æ€§ï¼‰
   - å…¼å®¹æ€§æµ‹è¯•ç‚¹ï¼ˆæµè§ˆå™¨ã€æ“ä½œç³»ç»Ÿã€è®¾å¤‡ï¼‰

3. **é›†æˆæµ‹è¯•ç‚¹æå–**ï¼š
   - ç³»ç»Ÿé—´æ¥å£æµ‹è¯•
   - æ•°æ®æµæµ‹è¯•
   - æœåŠ¡ä¾èµ–æµ‹è¯•

4. **éªŒæ”¶æµ‹è¯•ç‚¹æå–**ï¼š
   - åŸºäºç”¨æˆ·æ•…äº‹çš„éªŒæ”¶æµ‹è¯•
   - ä¸šåŠ¡åœºæ™¯æµ‹è¯•
   - ç«¯åˆ°ç«¯æµç¨‹æµ‹è¯•

5. **ä¸“é¡¹æµ‹è¯•ç‚¹æå–**ï¼š
   - è¾¹ç•Œæ¡ä»¶æµ‹è¯•
   - å¼‚å¸¸å¤„ç†æµ‹è¯•
   - å‹åŠ›æµ‹è¯•å’Œç¨³å®šæ€§æµ‹è¯•

6. **æµ‹è¯•è¦†ç›–åº¦åˆ†æ**ï¼š
   - éœ€æ±‚è¦†ç›–åº¦è¯„ä¼°
   - æµ‹è¯•æŠ€æœ¯åº”ç”¨åˆ†æ
   - è¦†ç›–åº¦ç¼ºå£è¯†åˆ«

7. **æµ‹è¯•ä¼˜å…ˆçº§å’Œé£é™©åˆ†æ**ï¼š
   - åŸºäºä¸šåŠ¡å½±å“çš„ä¼˜å…ˆçº§æ’åº
   - é£é™©è¯„ä¼°å’Œç¼“è§£æµ‹è¯•
   - å›å½’æµ‹è¯•ç‚¹è¯†åˆ«

8. **è‡ªåŠ¨åŒ–å¯è¡Œæ€§åˆ†æ**ï¼š
   - è‡ªåŠ¨åŒ–æµ‹è¯•ç‚¹è¯†åˆ«
   - è‡ªåŠ¨åŒ–å·¥å…·æ¨è
   - è‡ªåŠ¨åŒ–ç­–ç•¥åˆ¶å®š

è¯·ç¡®ä¿æå–ç»“æœä¸“ä¸šã€å…¨é¢ã€å¯æ‰§è¡Œï¼Œå¹¶æä¾›è¯¦ç»†çš„æµ‹è¯•è¦†ç›–åº¦åˆ†æã€‚
"""

    async def _run_ai_extraction(self, agent, prompt: str) -> str:
        """æ‰§è¡ŒAIæå–"""
        try:
            stream = agent.run_stream(task=prompt)
            async for event in stream:  # type: ignore
                # æµå¼æ¶ˆæ¯ï¼Œåªæ˜¯ä¸ºäº†åœ¨å‰ç«¯ç•Œé¢æµå¼æ˜¾ç¤º
                if isinstance(event, ModelClientStreamingChunkEvent):
                    # ä¸´æ—¶æ³¨é‡Šï¼Œä¸åœ¨å‰ç«¯æ˜¾ç¤ºæµå¼å†…å®¹
                    # await self.send_response(content=event.content, source=self.id.key)
                    continue

                # æœ€ç»ˆçš„å®Œæ•´ç»“æœ
                if isinstance(event, TaskResult):
                    messages = event.messages
                    # ä»æœ€åä¸€æ¡æ¶ˆæ¯ä¸­è·å–å®Œæ•´å†…å®¹
                    if messages and hasattr(messages[-1], 'content'):
                        return messages[-1].content

            # å¦‚æœæ²¡æœ‰è·å–åˆ°ç»“æœï¼Œè¿”å›é»˜è®¤å€¼
            return """
                {
                    "extraction_strategy": "comprehensive",
                    "coverage_analysis": {
                        "overall_coverage": 0.8,
                        "functional_coverage": 0.85,
                        "coverage_gaps": [],
                        "coverage_recommendations": []
                    },
                    "functional_test_points": [],
                    "non_functional_test_points": [],
                    "integration_test_points": [],
                    "acceptance_test_points": [],
                    "boundary_test_points": [],
                    "exception_test_points": [],
                    "security_test_points": [],
                    "performance_test_points": [],
                    "usability_test_points": [],
                    "compatibility_test_points": [],
                    "test_data_requirements": [],
                    "test_environment_requirements": [],
                    "test_dependency_matrix": [],
                    "test_priority_matrix": [],
                    "risk_based_test_points": [],
                    "regression_test_points": [],
                    "automation_feasibility": {
                        "overall_automation_score": 0.8,
                        "automation_strategy": "ä¼˜å…ˆè‡ªåŠ¨åŒ–æ ¸å¿ƒåŠŸèƒ½"
                    },
                    "test_execution_sequence": [],
                    "confidence_score": 0.8
                }
                """
        except Exception as e:
            logger.error(f"AIæå–æ‰§è¡Œå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return """
{
    "extraction_strategy": "basic",
    "coverage_analysis": {
        "overall_coverage": 0.5,
        "analysis_status": "failed"
    },
    "functional_test_points": [],
    "non_functional_test_points": [],
    "integration_test_points": [],
    "acceptance_test_points": [],
    "boundary_test_points": [],
    "exception_test_points": [],
    "security_test_points": [],
    "performance_test_points": [],
    "usability_test_points": [],
    "compatibility_test_points": [],
    "test_data_requirements": [],
    "test_environment_requirements": [],
    "test_dependency_matrix": [],
    "test_priority_matrix": [],
    "risk_based_test_points": [],
    "regression_test_points": [],
    "automation_feasibility": {
        "overall_automation_score": 0.5,
        "analysis_status": "failed"
    },
    "test_execution_sequence": [],
    "confidence_score": 0.5
}
"""

    def _parse_ai_extraction_result(
        self,
        ai_result: str,
        message: TestPointExtractionRequest
    ) -> TestPointExtractionResult:
        """è§£æAIæå–ç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            result_data = json.loads(ai_result.replace("```json", "").replace("```", ""))

            return TestPointExtractionResult(
                extraction_strategy=result_data.get("extraction_strategy", "comprehensive"),
                coverage_analysis=result_data.get("coverage_analysis", {}),
                functional_test_points=result_data.get("functional_test_points", []),
                non_functional_test_points=result_data.get("non_functional_test_points", []),
                integration_test_points=result_data.get("integration_test_points", []),
                acceptance_test_points=result_data.get("acceptance_test_points", []),
                boundary_test_points=result_data.get("boundary_test_points", []),
                exception_test_points=result_data.get("exception_test_points", []),
                security_test_points=result_data.get("security_test_points", []),
                performance_test_points=result_data.get("performance_test_points", []),
                usability_test_points=result_data.get("usability_test_points", []),
                compatibility_test_points=result_data.get("compatibility_test_points", []),
                test_data_requirements=result_data.get("test_data_requirements", []),
                test_environment_requirements=result_data.get("test_environment_requirements", []),
                test_dependency_matrix=result_data.get("test_dependency_matrix", []),
                test_priority_matrix=result_data.get("test_priority_matrix", []),
                risk_based_test_points=result_data.get("risk_based_test_points", []),
                regression_test_points=result_data.get("regression_test_points", []),
                automation_feasibility=result_data.get("automation_feasibility", {}),
                test_execution_sequence=result_data.get("test_execution_sequence", []),
                confidence_score=result_data.get("confidence_score", 0.5)
            )

        except json.JSONDecodeError:
            logger.warning("AIè¿”å›ç»“æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨é»˜è®¤è§£æ")
            return TestPointExtractionResult(
                extraction_strategy="basic",
                coverage_analysis={"overall_coverage": 0.3, "analysis_status": "json_parse_failed"},
                functional_test_points=[],
                non_functional_test_points=[],
                integration_test_points=[],
                acceptance_test_points=[],
                boundary_test_points=[],
                exception_test_points=[],
                security_test_points=[],
                performance_test_points=[],
                usability_test_points=[],
                compatibility_test_points=[],
                test_data_requirements=[],
                test_environment_requirements=[],
                test_dependency_matrix=[],
                test_priority_matrix=[],
                risk_based_test_points=[],
                regression_test_points=[],
                automation_feasibility={"overall_automation_score": 0.3, "analysis_status": "failed"},
                test_execution_sequence=[],
                confidence_score=0.3
            )

    async def _generate_test_cases_from_test_points(
        self,
        extraction_result: TestPointExtractionResult,
        message: TestPointExtractionRequest
    ) -> List[TestCaseData]:
        """ä»æµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        try:
            # åŸºäºåŠŸèƒ½æµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for test_point in extraction_result.functional_test_points:
                test_case = TestCaseData(
                    title=f"åŠŸèƒ½æµ‹è¯•: {test_point.get('name', 'æœªå‘½ååŠŸèƒ½æµ‹è¯•ç‚¹')}",
                    description=test_point.get("description", ""),
                    test_type=TestType.FUNCTIONAL,
                    test_level=TestLevel.SYSTEM,
                    priority=self._map_priority(test_point.get("priority", "P2")),
                    input_source=InputSource.MANUAL,
                    source_metadata={
                        "test_point_id": test_point.get("id"),
                        "test_point_type": "functional",
                        "test_technique": test_point.get("test_technique"),
                        "category": test_point.get("category"),
                        "automation_feasibility": test_point.get("automation_feasibility"),
                        "risk_level": test_point.get("risk_level"),
                        "business_impact": test_point.get("business_impact"),
                        "related_requirements": test_point.get("related_requirements", [])
                    },
                    ai_confidence=extraction_result.confidence_score
                )
                test_cases.append(test_case)

            # åŸºäºéåŠŸèƒ½æµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for test_point in extraction_result.non_functional_test_points:
                test_type = self._map_non_functional_test_type(test_point.get("type", "performance"))
                test_case = TestCaseData(
                    title=f"éåŠŸèƒ½æµ‹è¯•: {test_point.get('name', 'æœªå‘½åéåŠŸèƒ½æµ‹è¯•ç‚¹')}",
                    description=test_point.get("description", ""),
                    test_type=test_type,
                    test_level=TestLevel.SYSTEM,
                    priority=self._map_priority(test_point.get("priority", "P2")),
                    input_source=InputSource.MANUAL,
                    source_metadata={
                        "test_point_id": test_point.get("id"),
                        "test_point_type": "non_functional",
                        "nfr_type": test_point.get("type"),
                        "performance_criteria": test_point.get("performance_criteria"),
                        "load_conditions": test_point.get("load_conditions"),
                        "monitoring_metrics": test_point.get("monitoring_metrics", []),
                        "automation_feasibility": test_point.get("automation_feasibility")
                    },
                    ai_confidence=extraction_result.confidence_score
                )
                test_cases.append(test_case)

            # åŸºäºé›†æˆæµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for test_point in extraction_result.integration_test_points:
                test_case = TestCaseData(
                    title=f"é›†æˆæµ‹è¯•: {test_point.get('name', 'æœªå‘½åé›†æˆæµ‹è¯•ç‚¹')}",
                    description=test_point.get("description", ""),
                    test_type=TestType.INTERFACE,
                    test_level=TestLevel.INTEGRATION,
                    priority=self._map_priority(test_point.get("priority", "P1")),
                    input_source=InputSource.MANUAL,
                    source_metadata={
                        "test_point_id": test_point.get("id"),
                        "test_point_type": "integration",
                        "integration_type": test_point.get("integration_type"),
                        "components": test_point.get("components", []),
                        "data_flow": test_point.get("data_flow"),
                        "automation_feasibility": test_point.get("automation_feasibility")
                    },
                    ai_confidence=extraction_result.confidence_score
                )
                test_cases.append(test_case)

            # åŸºäºéªŒæ”¶æµ‹è¯•ç‚¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for test_point in extraction_result.acceptance_test_points:
                test_case = TestCaseData(
                    title=f"éªŒæ”¶æµ‹è¯•: {test_point.get('name', 'æœªå‘½åéªŒæ”¶æµ‹è¯•ç‚¹')}",
                    description=test_point.get("description", ""),
                    test_type=TestType.FUNCTIONAL,
                    test_level=TestLevel.ACCEPTANCE,
                    priority=self._map_priority(test_point.get("priority", "P0")),
                    input_source=InputSource.MANUAL,
                    source_metadata={
                        "test_point_id": test_point.get("id"),
                        "test_point_type": "acceptance",
                        "user_story": test_point.get("user_story"),
                        "acceptance_criteria": test_point.get("acceptance_criteria", []),
                        "business_scenarios": test_point.get("business_scenarios", []),
                        "stakeholders": test_point.get("stakeholders", [])
                    },
                    ai_confidence=extraction_result.confidence_score
                )
                test_cases.append(test_case)

            logger.info(f"ä»æµ‹è¯•ç‚¹ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return test_cases

        except Exception as e:
            logger.error(f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    def _map_non_functional_test_type(self, nfr_type: str) -> TestType:
        """æ˜ å°„éåŠŸèƒ½éœ€æ±‚ç±»å‹åˆ°æµ‹è¯•ç±»å‹"""
        mapping = {
            "performance": TestType.PERFORMANCE,
            "security": TestType.SECURITY,
            "usability": TestType.USABILITY,
            "reliability": TestType.FUNCTIONAL,
            "scalability": TestType.PERFORMANCE,
            "compatibility": TestType.COMPATIBILITY,
            "maintainability": TestType.FUNCTIONAL,
            "availability": TestType.FUNCTIONAL
        }
        return mapping.get(nfr_type.lower(), TestType.FUNCTIONAL)

    def _map_priority(self, priority_str: str) -> Priority:
        """æ˜ å°„ä¼˜å…ˆçº§"""
        mapping = {
            "high": Priority.P1,
            "medium": Priority.P2,
            "low": Priority.P3,
            "critical": Priority.P0,
            "P0": Priority.P0,
            "P1": Priority.P1,
            "P2": Priority.P2,
            "P3": Priority.P3,
            "P4": Priority.P4
        }
        return mapping.get(priority_str, Priority.P2)

    async def _send_to_test_case_generator(
        self,
        response: TestPointExtractionResponse,
        test_cases: List[TestCaseData]
    ):
        """å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“"""
        try:
            # ç›´æ¥å‘é€ TestPointExtractionResponseï¼Œè€Œä¸æ˜¯ TestCaseGenerationRequest
            await self.publish_message(
                response,
                topic_id=TopicId(type=TopicTypes.TEST_CASE_GENERATOR.value, source=self.id.key)
            )

            logger.info(f"å·²å‘é€æµ‹è¯•ç‚¹æå–å“åº”åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“: {response.session_id}")

        except Exception as e:
            logger.error(f"å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“å¤±è´¥: {str(e)}")
