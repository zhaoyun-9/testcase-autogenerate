"""
æ–‡æ¡£è§£ææ™ºèƒ½ä½“
è´Ÿè´£è§£æWordã€PDFç­‰éœ€æ±‚æ–‡æ¡£ï¼Œæå–æµ‹è¯•ç”¨ä¾‹ç›¸å…³ä¿¡æ¯
"""
import uuid
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

from autogen_agentchat.base import TaskResult
from autogen_agentchat.messages import ModelClientStreamingChunkEvent
from autogen_core import message_handler, type_subscription, MessageContext, TopicId
from loguru import logger
from pydantic import BaseModel, Field

from app.core.agents.base import BaseAgent
from app.core.types import TopicTypes, AgentTypes, AGENT_NAMES
from app.core.messages.test_case import (
    DocumentParseRequest, DocumentParseResponse,
    TestCaseGenerationRequest, TestCaseData
)
from app.core.enums import TestType, TestLevel, Priority, InputSource
from app.agents.database.requirement_saver_agent import RequirementSaveRequest


class DocumentParseResult(BaseModel):
    """æ–‡æ¡£è§£æç»“æœ"""
    document_type: str = Field(..., description="æ–‡æ¡£ç±»å‹")
    title: str = Field(..., description="æ–‡æ¡£æ ‡é¢˜")
    content: str = Field(..., description="æ–‡æ¡£å†…å®¹")
    sections: List[Dict[str, Any]] = Field(default_factory=list, description="æ–‡æ¡£ç« èŠ‚")
    requirements: List[Dict[str, Any]] = Field(default_factory=list, description="éœ€æ±‚åˆ—è¡¨")
    test_scenarios: List[Dict[str, Any]] = Field(default_factory=list, description="æµ‹è¯•åœºæ™¯")
    confidence_score: float = Field(0.0, description="è§£æç½®ä¿¡åº¦")


@type_subscription(topic_type=TopicTypes.DOCUMENT_PARSER.value)
class DocumentParserAgent(BaseAgent):
    """æ–‡æ¡£è§£ææ™ºèƒ½ä½“ï¼Œè´Ÿè´£è§£æå„ç§æ ¼å¼çš„éœ€æ±‚æ–‡æ¡£"""

    def __init__(self, model_client_instance=None, **kwargs):
        """åˆå§‹åŒ–æ–‡æ¡£è§£ææ™ºèƒ½ä½“"""
        super().__init__(
            agent_id=AgentTypes.DOCUMENT_PARSER.value,
            agent_name=AGENT_NAMES.get(AgentTypes.DOCUMENT_PARSER.value, "æ–‡æ¡£è§£ææ™ºèƒ½ä½“"),
            model_client_instance=model_client_instance,
            **kwargs
        )
        
        # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        self.supported_formats = {
            '.pdf': self._parse_pdf,
            '.docx': self._parse_docx,
            '.doc': self._parse_doc,
            '.txt': self._parse_txt,
            '.md': self._parse_markdown
        }
        
        logger.info(f"æ–‡æ¡£è§£ææ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ: {self.agent_name}")

    @message_handler
    async def handle_document_parse_request(
        self,
        message: DocumentParseRequest,
        ctx: MessageContext
    ) -> None:
        """å¤„ç†æ–‡æ¡£è§£æè¯·æ±‚"""
        start_time = datetime.now()

        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£è§£æè¯·æ±‚: {message.session_id}")

            # å‘é€å¼€å§‹å¤„ç†æ¶ˆæ¯
            await self.send_response(
                f"ğŸ” å¼€å§‹è§£ææ–‡æ¡£: {message.file_name}",
                region="process"
            )

            # å‘é€æ–‡æ¡£ä¿¡æ¯
            file_path = Path(message.file_path)
            file_size = file_path.stat().st_size if file_path.exists() else 0
            await self.send_response(
                f"ğŸ“„ æ–‡æ¡£ä¿¡æ¯: æ–‡ä»¶å¤§å° {file_size / 1024:.1f}KB, æ ¼å¼: {file_path.suffix}",
                region="info"
            )

            # è§£ææ–‡æ¡£
            await self.send_response("ğŸ”„ ç¬¬1æ­¥: å¼€å§‹è§£ææ–‡æ¡£å†…å®¹...", region="progress")
            parse_result = await self._parse_document(message)

            # å‘é€è§£æç»“æœç»Ÿè®¡
            await self.send_response(
                f"ğŸ“Š è§£æç»“æœ: æå– {len(parse_result.sections)} ä¸ªç« èŠ‚, {len(parse_result.requirements)} ä¸ªéœ€æ±‚",
                region="info",
                result={
                    "sections_count": len(parse_result.sections),
                    "requirements_count": len(parse_result.requirements),
                    "confidence_score": parse_result.confidence_score
                }
            )

            # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            await self.send_response("ğŸ”„ ç¬¬2æ­¥: åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹...", region="progress")
            test_cases = await self._generate_test_cases_from_document(
                parse_result, message
            )

            # å‘é€æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆç»“æœ
            await self.send_response(
                f"âœ… æˆåŠŸç”Ÿæˆ {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹",
                region="success",
                result={"test_cases_count": len(test_cases)}
            )

            # ä¿å­˜éœ€æ±‚åˆ°æ•°æ®åº“
            if parse_result.requirements:
                await self.send_response("ğŸ”„ ç¬¬3æ­¥: ä¿å­˜éœ€æ±‚ä¿¡æ¯åˆ°æ•°æ®åº“...", region="progress")
                await self._save_requirements_to_database(parse_result, message)

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()

            # æ„å»ºå“åº”
            response = DocumentParseResponse(
                session_id=message.session_id,
                document_id=str(uuid.uuid4()),
                file_name=message.file_name,
                file_path=message.file_path,
                parse_result=parse_result.model_dump(),
                test_cases=test_cases,
                processing_time=processing_time,
                created_at=datetime.now().isoformat()
            )

            # å‘é€å®Œæˆæ¶ˆæ¯
            await self.send_response(
                f"âœ… æ–‡æ¡£è§£æå®Œæˆ! å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’",
                is_final=False,
                region="success",
                result={
                    "processing_time": processing_time,
                    "total_test_cases": len(test_cases),
                    "total_requirements": len(parse_result.requirements),
                    "confidence_score": parse_result.confidence_score
                }
            )

            # å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“
            await self.send_response("ğŸ”„ è½¬å‘åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“è¿›è¡Œåç»­å¤„ç†...", region="info")
            await self._send_to_test_case_generator(response)

        except Exception as e:
            processing_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
            await self.send_response(
                f"âŒ æ–‡æ¡£è§£æå¤±è´¥: {str(e)} (å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’)",
                is_final=True,
                region="error",
                result={"processing_time": processing_time, "error": str(e)}
            )

    async def _parse_document(self, message: DocumentParseRequest) -> DocumentParseResult:
        """è§£ææ–‡æ¡£å†…å®¹"""
        try:
            file_path = Path(message.file_path)
            file_extension = file_path.suffix.lower()

            if file_extension not in self.supported_formats:
                await self.send_response(
                    f"âŒ ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_extension}",
                    region="error"
                )
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£æ ¼å¼: {file_extension}")

            await self.send_response(
                f"ğŸ“– æ­£åœ¨è§£æ {file_extension} æ ¼å¼æ–‡æ¡£...",
                region="progress"
            )

            # è°ƒç”¨å¯¹åº”çš„è§£ææ–¹æ³•
            parser_func = self.supported_formats[file_extension]
            content_start_time = datetime.now()

            # æå–é¡µé¢å†…å®¹
            content = await parser_func(file_path)
            content_time = (datetime.now() - content_start_time).total_seconds()

            await self.send_response(
                f"ğŸ“ å†…å®¹æå–å®Œæˆï¼Œè€—æ—¶ {content_time:.2f}ç§’ï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦",
                region="info",
                result={"content_extraction_time": content_time, "content_length": len(content)}
            )

            # ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹
            await self.send_response("ğŸ¤– å¼€å§‹AIæ™ºèƒ½åˆ†ææ–‡æ¡£å†…å®¹...", region="progress")
            ai_start_time = datetime.now()
            analysis_result = await self._analyze_document_content(content, message)
            ai_time = (datetime.now() - ai_start_time).total_seconds()

            await self.send_response(
                f"ğŸ§  AIåˆ†æå®Œæˆï¼Œè€—æ—¶ {ai_time:.2f}ç§’ï¼Œç½®ä¿¡åº¦: {analysis_result.confidence_score:.2f}",
                region="success",
                result={
                    "ai_analysis_time": ai_time,
                    "confidence_score": analysis_result.confidence_score,
                    "analysis_quality": "é«˜" if analysis_result.confidence_score > 0.8 else "ä¸­" if analysis_result.confidence_score > 0.6 else "ä½"
                }
            )

            return analysis_result

        except Exception as e:
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
            await self.send_response(
                f"âŒ æ–‡æ¡£è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                region="error"
            )
            raise
    async def _parse_pdf(self, file_path: Path) -> str:
        """è§£æPDFæ–‡æ¡£ï¼Œæå–å®Œæ•´æ–‡æœ¬å†…å®¹"""
        try:
            # å¯¼å…¥å¿…è¦çš„åº“
            import fitz  # PyMuPDF

            logger.info(f"å¼€å§‹è§£æPDFæ–‡æ¡£: {file_path}")

            # æ‰“å¼€PDFæ–‡æ¡£
            pdf_document = fitz.open(file_path)
            total_pages = len(pdf_document)

            await self.send_response(
                f"ğŸ“„ PDFæ–‡æ¡£ä¿¡æ¯: å…± {total_pages} é¡µï¼Œå¼€å§‹æ–‡æœ¬æå–...",
                region="info",
                result={"total_pages": total_pages, "parsing_method": "text_extraction"}
            )

            # å­˜å‚¨æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å†…å®¹
            all_text = []
            successful_pages = 0
            failed_pages = 0
            total_content_length = 0

            # é€é¡µæå–æ–‡æœ¬
            for page_num in range(total_pages):
                page_start_time = datetime.now()

                try:
                    await self.send_response(
                        f"ğŸ” æ­£åœ¨æå–ç¬¬ {page_num + 1}/{total_pages} é¡µæ–‡æœ¬...",
                        region="progress"
                    )

                    # è·å–é¡µé¢
                    page = pdf_document[page_num]

                    # æå–æ–‡æœ¬å†…å®¹
                    page_text = page.get_text()

                    # æ¸…ç†å’Œæ ¼å¼åŒ–æ–‡æœ¬
                    cleaned_text = self._clean_pdf_text(page_text)

                    page_time = (datetime.now() - page_start_time).total_seconds()

                    if cleaned_text.strip():
                        all_text.append({
                            'page_number': page_num + 1,
                            'content': cleaned_text.strip()
                        })
                        successful_pages += 1
                        total_content_length += len(cleaned_text)

                        await self.send_response(
                            f"âœ… ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–æˆåŠŸ (è€—æ—¶: {page_time:.2f}ç§’, å†…å®¹: {len(cleaned_text)} å­—ç¬¦)",
                            region="success"
                        )
                    else:
                        failed_pages += 1
                        await self.send_response(
                            f"âš ï¸ ç¬¬ {page_num + 1} é¡µæ— æ–‡æœ¬å†…å®¹ (è€—æ—¶: {page_time:.2f}ç§’)",
                            region="warning"
                        )

                    logger.info(f"ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(cleaned_text)}")

                except Exception as e:
                    failed_pages += 1
                    page_time = (datetime.now() - page_start_time).total_seconds()
                    logger.error(f"ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–å¤±è´¥: {str(e)}")
                    await self.send_response(
                        f"âŒ ç¬¬ {page_num + 1} é¡µæ–‡æœ¬æå–å¤±è´¥: {str(e)} (è€—æ—¶: {page_time:.2f}ç§’)",
                        region="error"
                    )
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue

            # å…³é—­PDFæ–‡æ¡£
            pdf_document.close()

            # å‘é€æå–ç»Ÿè®¡
            await self.send_response(
                f"ğŸ“Š PDFæ–‡æœ¬æå–ç»Ÿè®¡: æˆåŠŸ {successful_pages} é¡µ, å¤±è´¥ {failed_pages} é¡µ, æ€»å†…å®¹ {total_content_length} å­—ç¬¦",
                region="info",
                result={
                    "successful_pages": successful_pages,
                    "failed_pages": failed_pages,
                    "total_content_length": total_content_length,
                    "success_rate": (successful_pages / total_pages * 100) if total_pages > 0 else 0
                }
            )

            # æ•´åˆæ‰€æœ‰é¡µé¢å†…å®¹
            await self.send_response("ğŸ”„ æ­£åœ¨æ•´åˆé¡µé¢å†…å®¹...", region="progress")
            integration_start_time = datetime.now()
            final_content = self._integrate_pdf_text_content(all_text, file_path.name)
            integration_time = (datetime.now() - integration_start_time).total_seconds()

            await self.send_response(
                f"âœ… PDFæ–‡æœ¬è§£æå®Œæˆ! æœ‰æ•ˆé¡µé¢: {len(all_text)}, å†…å®¹æ•´åˆè€—æ—¶: {integration_time:.2f}ç§’",
                region="success",
                result={
                    "effective_pages": len(all_text),
                    "integration_time": integration_time,
                    "final_content_length": len(final_content)
                }
            )

            return final_content

        except ImportError as e:
            if "fitz" in str(e):
                raise ImportError("éœ€è¦å®‰è£… PyMuPDF: pip install PyMuPDF")
            else:
                raise ImportError(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {str(e)}")
        except Exception as e:
            logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
            raise

    def _clean_pdf_text(self, text: str) -> str:
        """æ¸…ç†å’Œæ ¼å¼åŒ–PDFæå–çš„æ–‡æœ¬"""
        try:
            if not text:
                return ""

            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            lines = text.split('\n')
            cleaned_lines = []

            for line in lines:
                # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
                line = line.strip()

                # è·³è¿‡ç©ºè¡Œ
                if not line:
                    continue

                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                line = ' '.join(line.split())

                # è·³è¿‡è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯é¡µçœ‰é¡µè„šæˆ–å™ªéŸ³ï¼‰
                if len(line) < 3:
                    continue

                cleaned_lines.append(line)

            # åˆå¹¶ç›¸å…³è¡Œï¼ˆå¤„ç†PDFä¸­çš„æ¢è¡Œé—®é¢˜ï¼‰
            merged_lines = []
            current_paragraph = ""

            for line in cleaned_lines:
                # å¦‚æœè¡Œä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰ç»“å°¾ï¼Œè®¤ä¸ºæ˜¯æ®µè½ç»“æŸ
                if line.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ':', 'ï¼š')):
                    current_paragraph += " " + line if current_paragraph else line
                    merged_lines.append(current_paragraph.strip())
                    current_paragraph = ""
                # å¦‚æœè¡Œçœ‹èµ·æ¥åƒæ ‡é¢˜ï¼ˆçŸ­ä¸”å¯èƒ½åŒ…å«æ•°å­—ï¼‰
                elif len(line) < 50 and any(char.isdigit() for char in line[:10]):
                    if current_paragraph:
                        merged_lines.append(current_paragraph.strip())
                        current_paragraph = ""
                    merged_lines.append(line)
                else:
                    current_paragraph += " " + line if current_paragraph else line

            # æ·»åŠ æœ€åçš„æ®µè½
            if current_paragraph:
                merged_lines.append(current_paragraph.strip())

            return '\n\n'.join(merged_lines)

        except Exception as e:
            logger.error(f"æ¸…ç†PDFæ–‡æœ¬å¤±è´¥: {str(e)}")
            return text  # è¿”å›åŸå§‹æ–‡æœ¬

    def _integrate_pdf_text_content(self, page_contents: list, filename: str) -> str:
        """æ•´åˆæ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å†…å®¹ä¸ºå®Œæ•´æ–‡æ¡£"""
        try:
            # æ„å»ºæ–‡æ¡£å¤´éƒ¨
            content_lines = [
                f"# {filename} - æ–‡æ¡£å†…å®¹",
                "",
                f"*æœ¬æ–‡æ¡£ç”±PDFæ–‡æœ¬æå–è‡ªåŠ¨ç”Ÿæˆï¼Œå…±{len(page_contents)}é¡µ*",
                "",
                "---",
                ""
            ]

            # å¦‚æœé¡µé¢è¾ƒå¤šï¼Œæ·»åŠ ç®€å•ç›®å½•
            if len(page_contents) > 5:
                content_lines.extend([
                    "## ç›®å½•",
                    ""
                ])
                for page_info in page_contents:
                    page_num = page_info['page_number']
                    # å°è¯•ä»å†…å®¹ä¸­æå–å‰å‡ ä¸ªè¯ä½œä¸ºç›®å½•é¡¹
                    content_preview = page_info['content'][:100].replace('\n', ' ')
                    if len(content_preview) > 50:
                        content_preview = content_preview[:50] + "..."
                    content_lines.append(f"- ç¬¬{page_num}é¡µ: {content_preview}")

                content_lines.extend(["", "---", ""])

            # æ·»åŠ æ¯é¡µå†…å®¹
            for page_info in page_contents:
                page_num = page_info['page_number']
                content = page_info['content']

                # æ·»åŠ é¡µé¢æ ‡é¢˜
                content_lines.extend([
                    f"## ç¬¬{page_num}é¡µ",
                    ""
                ])

                # æ·»åŠ é¡µé¢å†…å®¹
                content_lines.extend([
                    content,
                    "",
                    "---",
                    ""
                ])

            # æ·»åŠ æ–‡æ¡£å°¾éƒ¨
            content_lines.extend([
                "",
                f"*æ–‡æ¡£è§£æå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
            ])

            return "\n".join(content_lines)

        except Exception as e:
            logger.error(f"æ•´åˆPDFæ–‡æœ¬å†…å®¹å¤±è´¥: {str(e)}")
            # è¿”å›ç®€å•çš„å†…å®¹æ‹¼æ¥
            simple_content = f"# {filename}\n\n"
            for page_info in page_contents:
                simple_content += f"## ç¬¬{page_info['page_number']}é¡µ\n\n{page_info['content']}\n\n---\n\n"
            return simple_content

    async def _parse_pdf_with_multimodal(self, file_path: Path) -> str:
        """ä½¿ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹è§£æPDFæ–‡æ¡£"""
        try:
            # å¯¼å…¥å¿…è¦çš„åº“
            import fitz  # PyMuPDF
            from io import BytesIO
            from autogen_agentchat.messages import MultiModalMessage
            from autogen_core import Image as AGImage
            from PIL import Image

            logger.info(f"å¼€å§‹ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è§£æPDF: {file_path}")

            # æ‰“å¼€PDFæ–‡æ¡£
            pdf_document = fitz.open(file_path)
            total_pages = len(pdf_document)

            await self.send_response(
                f"ğŸ“„ PDFæ–‡æ¡£ä¿¡æ¯: å…± {total_pages} é¡µï¼Œå¼€å§‹å¤šæ¨¡æ€è§£æ...",
                region="info",
                result={"total_pages": total_pages, "parsing_method": "multimodal"}
            )

            # å­˜å‚¨æ¯é¡µè§£æç»“æœ
            page_contents = []
            successful_pages = 0
            failed_pages = 0
            total_content_length = 0

            # é€é¡µå¤„ç†
            for page_num in range(total_pages):
                page_start_time = datetime.now()

                try:
                    await self.send_response(
                        f"ğŸ” æ­£åœ¨è§£æç¬¬ {page_num + 1}/{total_pages} é¡µ...",
                        region="progress"
                    )

                    # è·å–é¡µé¢
                    page = pdf_document[page_num]

                    # å°†é¡µé¢è½¬æ¢ä¸ºå›¾ç‰‡
                    pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))  # 2å€ç¼©æ”¾æé«˜æ¸…æ™°åº¦
                    img_data = pix.tobytes("png")
                    img_size = len(img_data)

                    # è½¬æ¢ä¸ºPILå›¾ç‰‡
                    pil_image = Image.open(BytesIO(img_data))

                    # è½¬æ¢ä¸ºAGImage
                    ag_image = AGImage(pil_image)

                    # ä½¿ç”¨å¤šæ¨¡æ€æ¨¡å‹è§£æé¡µé¢å†…å®¹
                    page_content = await self._analyze_pdf_page_with_multimodal(
                        ag_image, page_num + 1, total_pages
                    )

                    page_time = (datetime.now() - page_start_time).total_seconds()

                    if page_content.strip():
                        page_contents.append({
                            'page_number': page_num + 1,
                            'content': page_content.strip()
                        })
                        successful_pages += 1
                        total_content_length += len(page_content)

                        await self.send_response(
                            f"âœ… ç¬¬ {page_num + 1} é¡µè§£ææˆåŠŸ (è€—æ—¶: {page_time:.2f}ç§’, å†…å®¹: {len(page_content)} å­—ç¬¦, å›¾ç‰‡: {img_size/1024:.1f}KB)",
                            region="success"
                        )
                    else:
                        failed_pages += 1
                        await self.send_response(
                            f"âš ï¸ ç¬¬ {page_num + 1} é¡µå†…å®¹ä¸ºç©º (è€—æ—¶: {page_time:.2f}ç§’)",
                            region="warning"
                        )

                    logger.info(f"ç¬¬ {page_num + 1} é¡µè§£æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(page_content)}")

                except Exception as e:
                    failed_pages += 1
                    page_time = (datetime.now() - page_start_time).total_seconds()
                    logger.error(f"ç¬¬ {page_num + 1} é¡µè§£æå¤±è´¥: {str(e)}")
                    await self.send_response(
                        f"âŒ ç¬¬ {page_num + 1} é¡µè§£æå¤±è´¥: {str(e)} (è€—æ—¶: {page_time:.2f}ç§’)",
                        region="error"
                    )
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue

            # å…³é—­PDFæ–‡æ¡£
            pdf_document.close()

            # å‘é€è§£æç»Ÿè®¡
            await self.send_response(
                f"ğŸ“Š PDFè§£æç»Ÿè®¡: æˆåŠŸ {successful_pages} é¡µ, å¤±è´¥ {failed_pages} é¡µ, æ€»å†…å®¹ {total_content_length} å­—ç¬¦",
                region="info",
                result={
                    "successful_pages": successful_pages,
                    "failed_pages": failed_pages,
                    "total_content_length": total_content_length,
                    "success_rate": (successful_pages / total_pages * 100) if total_pages > 0 else 0
                }
            )

            # æ•´åˆæ‰€æœ‰é¡µé¢å†…å®¹ä¸ºmarkdownæ ¼å¼
            await self.send_response("ğŸ”„ æ­£åœ¨æ•´åˆé¡µé¢å†…å®¹ä¸ºMarkdownæ ¼å¼...", region="progress")
            integration_start_time = datetime.now()
            markdown_content = await self._integrate_pdf_pages_to_markdown(page_contents, file_path.name)
            integration_time = (datetime.now() - integration_start_time).total_seconds()

            await self.send_response(
                f"âœ… PDFè§£æå®Œæˆ! æœ‰æ•ˆé¡µé¢: {len(page_contents)}, å†…å®¹æ•´åˆè€—æ—¶: {integration_time:.2f}ç§’",
                region="success",
                result={
                    "effective_pages": len(page_contents),
                    "integration_time": integration_time,
                    "final_content_length": len(markdown_content)
                }
            )

            return markdown_content

        except ImportError as e:
            if "fitz" in str(e):
                raise ImportError("éœ€è¦å®‰è£… PyMuPDF: pip install PyMuPDF")
            else:
                raise ImportError(f"ç¼ºå°‘å¿…è¦ä¾èµ–: {str(e)}")
        except Exception as e:
            logger.error(f"PDFè§£æå¤±è´¥: {str(e)}")
            raise

    async def _analyze_pdf_page_with_multimodal(self, ag_image, page_num: int, total_pages: int) -> str:
        """ä½¿ç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹åˆ†æPDFé¡µé¢"""
        try:
            from autogen_agentchat.messages import MultiModalMessage
            from app.agents.factory import agent_factory

            # åˆ›å»ºå¤šæ¨¡æ€åˆ†ææ™ºèƒ½ä½“ï¼ˆä½¿ç”¨åƒé—®æ¨¡å‹ï¼‰
            multimodal_agent = agent_factory.create_assistant_agent(
                name="pdf_page_analyzer",
                system_message=self._build_pdf_analysis_system_prompt(),
                model_client_type="qwenvl"  # ä½¿ç”¨åƒé—®VLæ¨¡å‹
            )

            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            analysis_prompt = f"""
è¯·ä»”ç»†åˆ†æè¿™å¼ PDFé¡µé¢å›¾ç‰‡ï¼ˆç¬¬{page_num}é¡µï¼Œå…±{total_pages}é¡µï¼‰ï¼Œæå–å…¶ä¸­çš„æ–‡æœ¬å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚

è¦æ±‚ï¼š
1. è¯†åˆ«å¹¶æå–æ‰€æœ‰å¯è§çš„æ–‡å­—å†…å®¹
2. ä¿æŒåŸæœ‰çš„å±‚æ¬¡ç»“æ„å’Œæ ¼å¼
3. è¯†åˆ«æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰å…ƒç´ 
4. å¦‚æœæœ‰å›¾è¡¨æˆ–å›¾åƒï¼Œè¯·æè¿°å…¶å†…å®¹å’Œä½œç”¨
5. è¾“å‡ºæ ¼å¼ä¸ºæ¸…æ™°çš„æ–‡æœ¬ï¼Œä¿æŒé€»è¾‘ç»“æ„

è¯·ç›´æ¥è¾“å‡ºé¡µé¢å†…å®¹ï¼Œä¸éœ€è¦é¢å¤–çš„è§£é‡Šè¯´æ˜ã€‚
"""

            # åˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            multi_modal_message = MultiModalMessage(
                content=[analysis_prompt, ag_image],
                source="user"
            )

            # æ‰§è¡Œåˆ†æ
            result = await multimodal_agent.run(task=multi_modal_message)

            # æå–ç»“æœå†…å®¹
            if result.messages and hasattr(result.messages[-1], 'content'):
                return result.messages[-1].content
            else:
                logger.warning(f"ç¬¬{page_num}é¡µå¤šæ¨¡æ€åˆ†ææœªè¿”å›æœ‰æ•ˆå†…å®¹")
                return ""

        except Exception as e:
            logger.error(f"ç¬¬{page_num}é¡µå¤šæ¨¡æ€åˆ†æå¤±è´¥: {str(e)}")
            return f"ç¬¬{page_num}é¡µè§£æå¤±è´¥: {str(e)}"

    def _build_pdf_analysis_system_prompt(self) -> str:
        """æ„å»ºPDFé¡µé¢åˆ†æç³»ç»Ÿæç¤º"""
        return """
ä½ æ˜¯ä¸“ä¸šçš„æ–‡æ¡£å†…å®¹è¯†åˆ«ä¸“å®¶ï¼Œæ“…é•¿ä»PDFé¡µé¢å›¾ç‰‡ä¸­å‡†ç¡®æå–æ–‡æœ¬å†…å®¹å’Œç»“æ„ä¿¡æ¯ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ä»”ç»†è§‚å¯ŸPDFé¡µé¢å›¾ç‰‡
2. è¯†åˆ«å¹¶æå–æ‰€æœ‰æ–‡å­—å†…å®¹
3. ä¿æŒåŸæœ‰çš„æ–‡æ¡£ç»“æ„å’Œæ ¼å¼
4. è¯†åˆ«æ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ã€è¡¨æ ¼ç­‰ä¸åŒç±»å‹çš„å†…å®¹
5. å¯¹äºå›¾è¡¨ã€å›¾åƒç­‰éæ–‡å­—å†…å®¹ï¼Œä»å¤šä¸ªè§’åº¦è¯¦ç»†æè¿°

è¾“å‡ºè¦æ±‚ï¼š
- ç›´æ¥è¾“å‡ºé¡µé¢çš„æ–‡æœ¬å†…å®¹
- ä¿æŒæ¸…æ™°çš„å±‚æ¬¡ç»“æ„
- ä½¿ç”¨é€‚å½“çš„æ ‡è®°æ¥åŒºåˆ†ä¸åŒç±»å‹çš„å†…å®¹
- ç¡®ä¿å†…å®¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
- ä¸è¦æ·»åŠ å¤šä½™çš„è§£é‡Šæˆ–è¯„è®º
"""

    async def _integrate_pdf_pages_to_markdown(self, page_contents: list, filename: str) -> str:
        """å°†æ‰€æœ‰é¡µé¢å†…å®¹æ•´åˆä¸ºå®Œæ•´çš„markdownæ–‡æ¡£"""
        try:
            # æ„å»ºmarkdownæ–‡æ¡£å¤´éƒ¨
            markdown_lines = [
                f"# {filename} - æ–‡æ¡£å†…å®¹è§£æ",
                "",
                f"*æœ¬æ–‡æ¡£ç”±AIå¤šæ¨¡æ€æ¨¡å‹è‡ªåŠ¨è§£æç”Ÿæˆï¼Œå…±{len(page_contents)}é¡µ*",
                "",
                "---",
                ""
            ]

            # æ·»åŠ ç›®å½•ï¼ˆå¦‚æœé¡µé¢è¾ƒå¤šï¼‰
            if len(page_contents) > 3:
                markdown_lines.extend([
                    "## ç›®å½•",
                    ""
                ])
                for page_info in page_contents:
                    page_num = page_info['page_number']
                    # å°è¯•ä»å†…å®¹ä¸­æå–æ ‡é¢˜ä½œä¸ºç›®å½•é¡¹
                    content_lines = page_info['content'].split('\n')
                    title = f"ç¬¬{page_num}é¡µ"
                    for line in content_lines[:5]:  # æ£€æŸ¥å‰5è¡Œå¯»æ‰¾å¯èƒ½çš„æ ‡é¢˜
                        line = line.strip()
                        if line and len(line) < 100:  # å¯èƒ½æ˜¯æ ‡é¢˜
                            title = f"ç¬¬{page_num}é¡µ: {line[:50]}..."
                            break
                    markdown_lines.append(f"- [{title}](#ç¬¬{page_num}é¡µ)")

                markdown_lines.extend(["", "---", ""])

            # æ·»åŠ æ¯é¡µå†…å®¹
            for page_info in page_contents:
                page_num = page_info['page_number']
                content = page_info['content']

                # æ·»åŠ é¡µé¢æ ‡é¢˜
                markdown_lines.extend([
                    f"## ç¬¬{page_num}é¡µ",
                    ""
                ])

                # å¤„ç†é¡µé¢å†…å®¹ï¼Œç¡®ä¿markdownæ ¼å¼æ­£ç¡®
                processed_content = self._process_page_content_for_markdown(content)
                markdown_lines.extend([
                    processed_content,
                    "",
                    "---",
                    ""
                ])

            # æ·»åŠ æ–‡æ¡£å°¾éƒ¨
            markdown_lines.extend([
                "",
                f"*æ–‡æ¡£è§£æå®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
            ])

            return "\n".join(markdown_lines)

        except Exception as e:
            logger.error(f"æ•´åˆPDFé¡µé¢å†…å®¹å¤±è´¥: {str(e)}")
            # è¿”å›ç®€å•çš„å†…å®¹æ‹¼æ¥
            simple_content = f"# {filename}\n\n"
            for page_info in page_contents:
                simple_content += f"## ç¬¬{page_info['page_number']}é¡µ\n\n{page_info['content']}\n\n---\n\n"
            return simple_content

    def _process_page_content_for_markdown(self, content: str) -> str:
        """å¤„ç†é¡µé¢å†…å®¹ï¼Œç¡®ä¿markdownæ ¼å¼æ­£ç¡®"""
        try:
            lines = content.split('\n')
            processed_lines = []

            for line in lines:
                line = line.strip()
                if not line:
                    processed_lines.append("")
                    continue

                # å¤„ç†å¯èƒ½çš„æ ‡é¢˜ï¼ˆä»¥æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦å¼€å¤´çš„è¡Œï¼‰
                if any(line.startswith(prefix) for prefix in ['1.', '2.', '3.', '4.', '5.', 'ä¸€ã€', 'äºŒã€', 'ä¸‰ã€', 'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰']):
                    processed_lines.append(f"### {line}")
                # å¤„ç†åˆ—è¡¨é¡¹
                elif line.startswith(('â€¢', 'Â·', '-', '*')):
                    processed_lines.append(f"- {line[1:].strip()}")
                # å¤„ç†æ™®é€šå†…å®¹
                else:
                    processed_lines.append(line)

            return "\n".join(processed_lines)

        except Exception as e:
            logger.error(f"å¤„ç†é¡µé¢å†…å®¹æ ¼å¼å¤±è´¥: {str(e)}")
            return content

    async def _parse_docx(self, file_path: Path) -> str:
        """è§£æDOCXæ–‡æ¡£"""
        try:
            from docx import Document
            
            doc = Document(file_path)
            content = ""
            
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            
            # è§£æè¡¨æ ¼
            for table in doc.tables:
                for row in table.rows:
                    row_text = "\t".join([cell.text for cell in row.cells])
                    content += row_text + "\n"
            
            return content.strip()
            
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£… python-docx: pip install python-docx")
        except Exception as e:
            logger.error(f"DOCXè§£æå¤±è´¥: {str(e)}")
            raise

    async def _parse_doc(self, file_path: Path) -> str:
        """è§£æDOCæ–‡æ¡£"""
        try:
            import subprocess
            import tempfile
            
            # ä½¿ç”¨LibreOfficeè½¬æ¢ä¸ºæ–‡æœ¬
            with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as tmp_file:
                cmd = [
                    'libreoffice', '--headless', '--convert-to', 'txt',
                    '--outdir', str(Path(tmp_file.name).parent),
                    str(file_path)
                ]
                
                subprocess.run(cmd, check=True, capture_output=True)
                
                txt_file = Path(tmp_file.name).with_suffix('.txt')
                if txt_file.exists():
                    content = txt_file.read_text(encoding='utf-8')
                    txt_file.unlink()  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    return content
                else:
                    raise Exception("DOCè½¬æ¢å¤±è´¥")
                    
        except Exception as e:
            logger.error(f"DOCè§£æå¤±è´¥: {str(e)}")
            # å›é€€åˆ°ç®€å•æ–‡æœ¬è¯»å–
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            except:
                raise Exception(f"DOCæ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

    async def _parse_txt(self, file_path: Path) -> str:
        """è§£æTXTæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            for encoding in ['gbk', 'gb2312', 'latin-1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            raise Exception("æ— æ³•è¯†åˆ«æ–‡æ¡£ç¼–ç ")

    async def _parse_markdown(self, file_path: Path) -> str:
        """è§£æMarkdownæ–‡æ¡£"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"Markdownè§£æå¤±è´¥: {str(e)}")
            raise

    async def _analyze_document_content(
        self, 
        content: str, 
        message: DocumentParseRequest
    ) -> DocumentParseResult:
        """ä½¿ç”¨AIåˆ†ææ–‡æ¡£å†…å®¹"""
        try:
            # åˆ›å»ºAIåˆ†ææ™ºèƒ½ä½“
            agent = self._create_document_analysis_agent()
            
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = self._build_document_analysis_prompt(content, message)
            
            # æ‰§è¡ŒAIåˆ†æ
            analysis_result = await self._run_ai_analysis(agent, analysis_prompt)
            
            # è§£æAIå“åº”
            return self._parse_ai_analysis_result(analysis_result, content)
            
        except Exception as e:
            logger.error(f"AIæ–‡æ¡£åˆ†æå¤±è´¥: {str(e)}")
            # è¿”å›åŸºç¡€è§£æç»“æœ
            return DocumentParseResult(
                document_type="unknown",
                title=message.file_name,
                content=content,
                sections=[],
                requirements=[],
                test_scenarios=[],
                confidence_score=0.5
            )

    def _create_document_analysis_agent(self):
        """åˆ›å»ºæ–‡æ¡£åˆ†ææ™ºèƒ½ä½“"""
        from app.agents.factory import agent_factory

        return agent_factory.create_assistant_agent(
            name="document_analyzer",
            system_message=self._build_document_analysis_system_prompt(),
            model_client_type="deepseek"
        )

    def _build_document_analysis_system_prompt(self) -> str:
        """æ„å»ºæ–‡æ¡£åˆ†æç³»ç»Ÿæç¤º"""
        return """
ä½ æ˜¯ä¸“ä¸šçš„éœ€æ±‚æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»å„ç§æ–‡æ¡£ä¸­æå–æµ‹è¯•ç›¸å…³ä¿¡æ¯ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ†ææ–‡æ¡£ç»“æ„å’Œå†…å®¹
2. è¯†åˆ«åŠŸèƒ½éœ€æ±‚å’ŒéåŠŸèƒ½éœ€æ±‚
3. æå–æµ‹è¯•åœºæ™¯å’Œç”¨ä¾‹
4. ç”Ÿæˆç»“æ„åŒ–çš„åˆ†æç»“æœ

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{
    "document_type": "éœ€æ±‚æ–‡æ¡£ç±»å‹",
    "title": "æ–‡æ¡£æ ‡é¢˜",
    "sections": [
        {
            "title": "ç« èŠ‚æ ‡é¢˜",
            "content": "ç« èŠ‚å†…å®¹æ‘˜è¦",
            "level": 1
        }
    ],
    "requirements": [
        {
            "id": "éœ€æ±‚ID",
            "title": "éœ€æ±‚æ ‡é¢˜",
            "description": "éœ€æ±‚æè¿°",
            "type": "functional/non-functional",
            "priority": "high/medium/low"
        }
    ],
    "test_scenarios": [
        {
            "title": "æµ‹è¯•åœºæ™¯æ ‡é¢˜",
            "description": "æµ‹è¯•åœºæ™¯æè¿°",
            "requirements": ["å…³è”éœ€æ±‚ID"],
            "test_type": "functional/performance/security/compatibility",
            "priority": "P0/P1/P2/P3/P4"
        }
    ],
    "confidence_score": 0.95
}

æ³¨æ„ï¼š
- ä»”ç»†åˆ†ææ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®è¯†åˆ«éœ€æ±‚å’Œæµ‹è¯•ç‚¹
- ä¼˜å…ˆè¯†åˆ«æ˜ç¡®çš„åŠŸèƒ½éœ€æ±‚å’Œä¸šåŠ¡æµç¨‹
- ä¸ºæ¯ä¸ªæµ‹è¯•åœºæ™¯åˆ†é…åˆé€‚çš„æµ‹è¯•ç±»å‹å’Œä¼˜å…ˆçº§
- ç¡®ä¿è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼,å»æ‰ ```json å’Œ ```
"""

    def _build_document_analysis_prompt(
        self, 
        content: str, 
        message: DocumentParseRequest
    ) -> str:
        """æ„å»ºæ–‡æ¡£åˆ†ææç¤º"""
        return f"""
è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œæå–æµ‹è¯•ç›¸å…³ä¿¡æ¯ï¼š

æ–‡æ¡£åç§°: {message.file_name}
æ–‡æ¡£ç±»å‹: {message.document_type or "æœªçŸ¥"}
åˆ†æç›®æ ‡: {message.analysis_target or "ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"}

æ–‡æ¡£å†…å®¹ï¼š
{content[:10000]}  # é™åˆ¶å†…å®¹é•¿åº¦é¿å…tokenè¶…é™

è¯·æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œè¯†åˆ«æ‰€æœ‰å¯æµ‹è¯•çš„åŠŸèƒ½ç‚¹å’Œä¸šåŠ¡æµç¨‹ï¼Œç”Ÿæˆå¯¹åº”çš„æµ‹è¯•åœºæ™¯ã€‚
"""

    async def _run_ai_analysis(self, agent, prompt: str) -> str:
        """æ‰§è¡ŒAIåˆ†æ"""
        try:
            stream = agent.run_stream(task=prompt)
            async for event in stream:  # type: ignore
                # æµå¼æ¶ˆæ¯ï¼Œåªæ˜¯ä¸ºäº†åœ¨å‰ç«¯ç•Œé¢æµå¼æ˜¾ç¤º
                if isinstance(event, ModelClientStreamingChunkEvent):
                    # ä¸´æ—¶æ³¨é‡Šï¼Œä¸åœ¨å‰ç«¯æ˜¾ç¤ºæµå¼å†…å®¹
                    # await self.send_response(content=event.content, source=self.id.key)
                    continue

                # æœ€ç»ˆçš„å®Œæ•´ç»“æœ
                if isinstance(event, TaskResult):
                    messages = event.messages
                    # ä»æœ€åä¸€æ¡æ¶ˆæ¯ä¸­è·å–å®Œæ•´å†…å®¹
                    if messages and hasattr(messages[-1], 'content'):
                        return messages[-1].content

            # å¦‚æœæ²¡æœ‰è·å–åˆ°ç»“æœï¼Œè¿”å›é»˜è®¤å€¼
            return """
                {
                    "document_type": "åŠŸèƒ½éœ€æ±‚æ–‡æ¡£",
                    "title": "ç³»ç»Ÿéœ€æ±‚è§„æ ¼è¯´æ˜ä¹¦",
                    "sections": [],
                    "requirements": [],
                    "test_scenarios": [],
                    "confidence_score": 0.8
                }
                """
        except Exception as e:
            logger.error(f"AIåˆ†ææ‰§è¡Œå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤ç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return """
{
    "document_type": "åŠŸèƒ½éœ€æ±‚æ–‡æ¡£",
    "title": "è§£æå¤±è´¥",
    "sections": [],
    "requirements": [],
    "test_scenarios": [],
    "confidence_score": 0.5
}
"""

    def _parse_ai_analysis_result(
        self, 
        ai_result: str, 
        original_content: str
    ) -> DocumentParseResult:
        """è§£æAIåˆ†æç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            result_data = json.loads(ai_result.replace("```json", "").replace("```", ""))
            
            return DocumentParseResult(
                document_type=result_data.get("document_type", "unknown"),
                title=result_data.get("title", "æœªçŸ¥æ–‡æ¡£"),
                content=original_content,
                sections=result_data.get("sections", []),
                requirements=result_data.get("requirements", []),
                test_scenarios=result_data.get("test_scenarios", []),
                confidence_score=result_data.get("confidence_score", 0.5)
            )
            
        except json.JSONDecodeError:
            logger.warning("AIè¿”å›ç»“æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œä½¿ç”¨é»˜è®¤è§£æ")
            return DocumentParseResult(
                document_type="unknown",
                title="è§£æå¤±è´¥",
                content=original_content,
                sections=[],
                requirements=[],
                test_scenarios=[],
                confidence_score=0.3
            )

    async def _generate_test_cases_from_document(
        self, 
        parse_result: DocumentParseResult,
        message: DocumentParseRequest
    ) -> List[TestCaseData]:
        """ä»æ–‡æ¡£è§£æç»“æœç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []
        
        try:
            # åŸºäºæµ‹è¯•åœºæ™¯ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            for scenario in parse_result.test_scenarios:
                test_case = TestCaseData(
                    title=scenario.get("title", "æœªå‘½åæµ‹è¯•ç”¨ä¾‹"),
                    description=scenario.get("description", ""),
                    test_type=self._map_test_type(scenario.get("test_type", "functional")),
                    test_level=TestLevel.SYSTEM,
                    priority=self._map_priority(scenario.get("priority", "P2")),
                    input_source=InputSource.DOCUMENT,
                    ai_confidence=parse_result.confidence_score
                )
                test_cases.append(test_case)
            
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æµ‹è¯•åœºæ™¯ï¼ŒåŸºäºéœ€æ±‚ç”Ÿæˆ
            if not test_cases and parse_result.requirements:
                for req in parse_result.requirements:
                    test_case = TestCaseData(
                        title=f"æµ‹è¯•éœ€æ±‚: {req.get('title', 'æœªå‘½åéœ€æ±‚')}",
                        description=req.get("description", ""),
                        test_type=TestType.FUNCTIONAL,
                        test_level=TestLevel.SYSTEM,
                        priority=self._map_priority(req.get("priority", "medium")),
                        input_source=InputSource.DOCUMENT,
                        source_metadata={
                            "document_name": message.file_name,
                            "requirement_id": req.get("id")
                        },
                        ai_confidence=parse_result.confidence_score
                    )
                    test_cases.append(test_case)
            
            logger.info(f"ä»æ–‡æ¡£ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
            return test_cases
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
            return []

    def _map_test_type(self, test_type_str: str) -> TestType:
        """æ˜ å°„æµ‹è¯•ç±»å‹"""
        mapping = {
            "functional": TestType.FUNCTIONAL,
            "performance": TestType.PERFORMANCE,
            "security": TestType.SECURITY,
            "compatibility": TestType.COMPATIBILITY,
            "usability": TestType.USABILITY,
            "interface": TestType.INTERFACE,
            "database": TestType.DATABASE
        }
        return mapping.get(test_type_str.lower(), TestType.FUNCTIONAL)

    def _map_priority(self, priority_str: str) -> Priority:
        """æ˜ å°„ä¼˜å…ˆçº§"""
        mapping = {
            "high": Priority.P1,
            "medium": Priority.P2,
            "low": Priority.P3,
            "P0": Priority.P0,
            "P1": Priority.P1,
            "P2": Priority.P2,
            "P3": Priority.P3,
            "P4": Priority.P4
        }
        return mapping.get(priority_str, Priority.P2)

    async def _send_to_test_case_generator(self, response: DocumentParseResponse):
        """å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“"""
        try:
            generation_request = TestCaseGenerationRequest(
                session_id=response.session_id,
                source_type="document",
                source_data=response.model_dump(),
                test_cases=response.test_cases,
                generation_config={
                    "auto_save": True,
                    "generate_mind_map": True
                }
            )
            
            await self.publish_message(
                generation_request,
                topic_id=TopicId(type=TopicTypes.TEST_CASE_GENERATOR.value, source=self.id.key)
            )
            
            logger.info(f"å·²å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“: {response.session_id}")

        except Exception as e:
            logger.error(f"å‘é€åˆ°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆæ™ºèƒ½ä½“å¤±è´¥: {str(e)}")

    async def _save_requirements_to_database(
        self,
        parse_result: DocumentParseResult,
        message: DocumentParseRequest
    ) -> None:
        """ä¿å­˜éœ€æ±‚åˆ°æ•°æ®åº“"""
        try:
            await self.send_response(
                f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(parse_result.requirements)} ä¸ªéœ€æ±‚åˆ°æ•°æ®åº“...",
                region="process"
            )

            # æ„å»ºéœ€æ±‚ä¿å­˜è¯·æ±‚
            requirement_save_request = RequirementSaveRequest(
                session_id=message.session_id,
                document_id=str(uuid.uuid4()),
                file_name=message.file_name,
                file_path=message.file_path,
                requirements=parse_result.requirements,
                project_id=None,  # ä½¿ç”¨é»˜è®¤é¡¹ç›®
                ai_model_info={
                    "model": "deepseek-chat",
                    "generation_time": datetime.now().isoformat(),
                    "agent_version": "1.0",
                    "session_id": message.session_id,
                    "confidence_score": parse_result.confidence_score
                }
            )

            # å‘é€åˆ°éœ€æ±‚å­˜å‚¨æ™ºèƒ½ä½“
            await self.publish_message(
                requirement_save_request,
                topic_id=TopicId(type=TopicTypes.REQUIREMENT_SAVER.value, source=self.id.key)
            )

            logger.info(f"å·²å‘é€éœ€æ±‚ä¿å­˜è¯·æ±‚åˆ°éœ€æ±‚å­˜å‚¨æ™ºèƒ½ä½“: {message.session_id}")

        except Exception as e:
            logger.error(f"ä¿å­˜éœ€æ±‚åˆ°æ•°æ®åº“å¤±è´¥: {str(e)}")
            await self.send_response(
                f"âš ï¸ éœ€æ±‚ä¿å­˜å¤±è´¥: {str(e)}",
                region="warning"
            )
